{"./":{"url":"./","title":"Introduction","keywords":"","body":" 主要参考： 崔庆才: 静觅博客 B 站: 龙王山小青椒 : Python网络爬虫 CSDN: bqw的博客 : Python爬虫 CSDN: King : Web Crawler CSDN: Kosmoo的博客 : Python爬虫 CSDN: V_lq6h的博客 : Spider CSDN: Pylemon : python爬虫入门到精通 Selenium with Python中文翻译文档 博客园:隔壁的王先森 : python---selenium Update time： 2020-08-14 "},"Chapter1/":{"url":"Chapter1/","title":"基础","keywords":"","body":"基础 Update time： 2020-05-26 "},"Chapter1/HTTP基本原理.html":{"url":"Chapter1/HTTP基本原理.html","title":"HTTP基本原理","keywords":"","body":"HTTP基本原理 参考 HTTP基本原理 Update time： 2020-08-13 "},"Chapter1/requests库.html":{"url":"Chapter1/requests库.html","title":"requests库","keywords":"","body":"requests库 安装 使用pip进行安装 要安装 requests，最方便快捷的方法是使用 pip 进行安装。 pip install requests 如果还没有安装pip，这个链接 Properly Installing Python 详细介绍了在各种平台下如何安装 python 以及 setuptools，pip，virtualenv 等常用的 python 工具，可以参考其中的步骤来进行安装。 发送请求与传递参数 使用 Requests 发送网络请求非常简单。 一开始要导入 Requests 模块： >>> import requests 然后，尝试获取某个网页。本例子中，我们来获取 Github 的公共时间线： >>> r = requests.get('https://api.github.com/events') 现在，我们有一个名为 r 的 Response 对象。我们可以从这个对象中获取所有我们想要的信息。 Requests 简便的 API 意味着所有 HTTP 请求类型都是显而易见的。例如，你可以这样发送一个 HTTP POST 请求： >>> r = requests.post('http://httpbin.org/post', data = {'key':'value'}) 漂亮，对吧？那么其他 HTTP 请求类型：PUT，DELETE，HEAD 以及 OPTIONS 又是如何的呢？都是一样的简单： >>> r = requests.put('http://httpbin.org/put', data = {'key':'value'}) >>> r = requests.delete('http://httpbin.org/delete') >>> r = requests.head('http://httpbin.org/get') >>> r = requests.options('http://httpbin.org/get') 都很不错吧，但这也仅是 Requests 的冰山一角呢。 传递 URL 参数 你也许经常想为 URL 的查询字符串(query string)传递某种数据。如果你是手工构建 URL，那么数据会以键/值对的形式置于 URL 中，跟在一个问号的后面。例如， httpbin.org/get?key=val。 Requests 允许你使用 params 关键字参数，以一个字符串字典来提供这些参数。举例来说，如果你想传递 key1=value1 和 key2=value2 到 httpbin.org/get ，那么你可以使用如下代码： >>> payload = {'key1': 'value1', 'key2': 'value2'} >>> r = requests.get(\"http://httpbin.org/get\", params=payload) 通过打印输出该 URL，你能看到 URL 已被正确编码： >>> print(r.url) http://httpbin.org/get?key2=value2&key1=value1 注意字典里值为 None 的键都不会被添加到 URL 的查询字符串里。 你还可以将一个列表作为值传入： >>> payload = {'key1': 'value1', 'key2': ['value2', 'value3']} >>> r = requests.get('http://httpbin.org/get', params=payload) >>> print(r.url) http://httpbin.org/get?key1=value1&key2=value2&key2=value3 参数说明 http 请求 get 与 post 是最常用的，url 为必选参数，常用常用参数有params、data、json、files、timeout、headers、cookies；其他基本用不到的有verify，cert，auth，allow_redirects，proxies，hooks，stream。 下面列表对具体的参数说明： 重点在 params、data、json、files、timeout，其次headers、cookies，其他略过就可以。 Response对象 get请求是最简单的、发送的数据量比较小。 使用 requests方法后，会返回一个response对象，其存储了服务器响应的内容，如上实例中已经提到的 r.text、r.status_code…… 获取文本方式的响应体实例：当你访问 r.text 之时，会使用其响应的文本编码进行解码，并且你可以修改其编码让 r.text 使用自定义的编码进行解码。 r = requests.get('http://www.itwhy.org') print(r.text, '\\n{}\\n'.format('*'*79), r.encoding) r.encoding = 'GBK' print(r.text, '\\n{}\\n'.format('*'*79), r.encoding) 其他响应： r.status_code # 响应状态码 r.encoding # 获取网页编码 r.raw #返回原始响应体，也就是 urllib 的 response 对象，使用 r.raw.read() 读取 r.content #字节方式的响应体，会自动为你解码 gzip 和 deflate 压缩 r.text #字符串方式的响应体，会自动根据响应头部的字符编码进行解码 r.headers #以字典对象存储服务器响应头，但是这个字典比较特殊，字典键不区分大小写，若键不存在则返回None #*特殊方法*# r.json() #Requests中内置的JSON解码器 r.raise_for_status() #失败请求(非200响应)抛出异常 示例2.1: 带多个参数的请求，返回文本数据 import requests # 带参数的GET请求,timeout请求超时时间 params = {'key1': 'python', 'key2': 'java'} r = requests.get(url='http://httpbin.org/get', params=params, timeout=3) # 注意观察url地址，它已经将参数拼接起来 print('URL地址：', r.url) # 响应状态码，成功返回200，失败40x或50x print('请求状态码：', r.status_code) print('header信息:', r.headers) print('cookie信息：', r.cookies) print('响应的数据：', r.text) # 如响应是json数据 ，可以使用 r.json()自动转换为dict print('响应json数据', r.json()) 示例2.2：get 返回二进制数据，如图片。 from PIL import Image from io import BytesIO import requests # 请求获取图片并保存 r = requests.get('https://pic3.zhimg.com/247d9814fec770e2c85cc858525208b2_is.jpg') i = Image.open(BytesIO(r.content)) # i.show() # 查看图片 # 将图片保存 with open('img.jpg', 'wb') as fd: for chunk in r.iter_content(): fd.write(chunk) post请求，上传表单，文本，文件\\图片 post 请求比 get 复杂，请求的数据有多种多样，有表单(form-data)，文本(json\\xml等)，文件流（图片\\文件）等。 要发送POST请求，只需要把get()方法变成post()，然后传入data参数作为POST请求的数据： 表单形式提交的post请求，只需要将数据传递给post()方法的data参数。见示例3.1. json文本形式提交的post请求，一种方式是将json数据dumps后传递给data参数，另一种方式就是直接将json数据传递给post()方法的json参数。见示例3.2. 单个文件提交的post请求，将文件流给post()方法的files参数。见示例3.3。 多个文件提交的post请求，将文件设到一个元组的列表中，其中元组结构为 (form_field_name, file_info)；然后将数据传递给post()方法的files。见示例3.4 示例3.1：post 表单请求 import requests, json # 带参数表单类型post请求 data={'custname': 'woodman','custtel':'13012345678','custemail':'woodman@11.com', 'size':'small'} r = requests.post('http://httpbin.org/post', data=data) print('响应数据：', r.text) 示例3.2：post json请求 # json数据请求 url = 'https://api.github.com/some/endpoint' payload = {'some': 'data'} # 可以使用json.dumps(dict) 对编码进行编译 r = requests.post(url, data=json.dumps(payload)) print('响应数据：', r.text) # 可以直接使用json参数传递json数据 r = requests.post(url, json=payload) print('响应数据：', r.text) 示例3.3：post提交单个文件 # 上传单个文件 url = 'http://httpbin.org/post' # 注意文件打开的模式，使用二进制模式不容易发生错误 files = {'file': open('report.txt', 'rb')} # 也可以显式地设置文件名，文件类型和请求头 # files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})} r = requests.post(url, files=files) r.encoding = 'utf-8' print(r.text) 注意：文件的上传使用二进制打开不容易报错。 示例3.4：上传多个文件 url = 'http://httpbin.org/post' multiple_files = [ ('images', ('foo.png', open('foo.png', 'rb'), 'image/png')), ('images', ('bar.png', open('bar.png', 'rb'), 'image/png'))] r = requests.post(url, files=multiple_files) print(r.text) 定制请求头 获取 get 与 post 请求响应的 header 与 cookie 分别使用 r.headers 与r.cookies 。如果提交请求数据是对 header 与 cookie 有修改，需要在get()与post()方法中加入 headers 或 cookies 参数，它们值的类型都是字典 示例4.1：定制请求头header import requests url = 'https://api.github.com/some/endpoint' headers = {'user-agent': 'my-app/0.0.1'} r = requests.get(url, headers=headers) print(r.headers) # 获取响应数据的header信息 注意：requests自带headers管理，一般情况下不需要设置header信息。Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 示例4.2：定制cookie信息 # 直接以字典型时传递cookie url = 'http://httpbin.org/cookies' cookies = {\"cookies_are\":'working'} r = requests.get(url, cookies=cookies) # 获取响应的cookie信息，返回结果是RequestsCookieJar对象 print(r.cookies) print(r.text) session与cookie存储 会话对象requests.Session能够跨请求地保持某些参数，比如cookies，即在同一个Session实例发出的所有请求都保持同一个cookies,而requests模块每次会自动处理cookies，这样就很方便地处理登录时的cookies问题。 如果你向同一主机发送多个请求，每个请求对象让你能够跨请求保持session和cookie信息，这时我们要使用到requests的Session()来保持回话请求的cookie和session与服务器的相一致。 import requests url = \"http://www.renren.com/PLogin.do\" data = {\"email\":\"970138074@qq.com\",'password':\"pythonspider\"} headers = { 'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\" } # 登录 session = requests.session() session.post(url,data=data,headers=headers) # 访问大鹏个人中心 resp = session.get('http://www.renren.com/880151247/profile') print(resp.text) requests请求返回对象Response的常用方法 requests.get(url) 与 requests.post(url) 的返回对象为Response 类对象。 Response响应类常用属性与方法： Response.url 请求url，[见示例2.1] Response.status_code 响应状态码，[见示例2.1] Response.text 获取响应内容，[见示例2.1] Response.content 以字节形式获取响应提，多用于非文本请求，[见示例2.2] Response.json() 活动响应的JSON内容，[见示例2.1] Response.ok 请求是否成功，status_code 参考 python3之requests Update time： 2020-08-13 "},"Chapter1/requests库：代理proxies.html":{"url":"Chapter1/requests库：代理proxies.html","title":"requests库：代理proxies","keywords":"","body":"requests库：代理proxies 当频繁请求一个网站时，对方会认为攻击或者盗取数据，禁用1p是反制的有效手段。 如何破解这个问题？ 推荐方案就是降低爬虫请求的频率，不要对别人的服务器造成压力。 使用代理P 什么是代理： 代理相当于一个连接客户端和远程服务器的“中转站 当我们向服务器提出请求后，代理服务器先获取用户的请求，再将服务请求转交至远程服务器，并将远程服务器反馈的结果再转交给客户端。这就相当于，和服务端打交道的是代理服务器 使用场景 目标网站会根据P的访问频度判断如果超过正常频度，就会限制该P，拒绝访问，这个时候就需要使用代理IP，来伪装真实 IP 身份 作用 使用代理之后，远程服务器只能探测到代理服务器的P地址而不是上网者的真实P，从而达到隐藏上网者IP地址的目的，保障了上网者的网络安全 代理的作用： 一般分为透明代理、普通匿名代理和高匿代理 透明代理：远程服务器可以知道你使用了代理，并且透明也会将本机真实的P发送至远程服务器，因此无法达到隐藏身份的目的 匿名代理：远程服务器可以知道你使用了代理，但不知道你的真实 IP 高匿代理：高匿名代理隐藏了你的真实 IP，同时访问对象也不知道你使用了代理，因此隐蔽度最高 代理网站： 西祠代理、快代理、豌豆代理、蘑菇代理等等。 测试本机 IP http://httpbin.org/get http://www.ip138.com/ 测试： import requests # 请求的 url, 可以查看本机 IP url = \"http://httpbin.org/get\" headers = { \"user-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36' } # 发送 get 请求 res = requests.get(url,headers=headers) # 查看状态码 res.status_code # 200 定义代理 IP 链接超时： 可以去代理网站重新找新的 IP (可以利用爬虫爬取，也可以使用代理提供的 API 进行获取) 直到获取可以访问的代理 IP 并查看其返回的状态码和 IP # 查看状态码 res.status_code data = res.json() # 获取 IP, 如果能够正常获取 IP, 说明这个 IP 可以正常使用 data['origin'] Update time： 2020-08-11 "},"Chapter1/requestsSession应用.html":{"url":"Chapter1/requestsSession应用.html","title":"requests.Session应用","keywords":"","body":"requests.Session应用 session与cookie存储 会话对象requests.Session能够跨请求地保持某些参数，比如cookies，即在同一个Session实例发出的所有请求都保持同一个cookies,而requests模块每次会自动处理cookies，这样就很方便地处理登录时的cookies问题。 如果你向同一主机发送多个请求，每个请求对象让你能够跨请求保持session和cookie信息，这时我们要使用到requests的Session()来保持回话请求的cookie和session与服务器的相一致。 模拟登录V站 本篇文章的任务是利用request.Session模拟登录V2EX（http://www.v2ex.com/）这个网站，即V站。 工具： Python 3.5，BeautifulSoup模块，requests模块，Chrome 这个网站登录的时候抓到的数据如下： 其中用户名(u)、密码(p)都是明文传输的，很方便。once的话从分析登录URL： http://www.v2ex.com/signin 的源文件（下图）可以看出，应该是每次登录的特有数据，我们需要提前把它抓出来再放到Form Data里面POST给网站。 抓出来还是老方法，用BeautifulSoup神器即可。这里又学到一种抓标签里面元素的方法，比如抓上面的\"value\",用soup.find('input',{'name':'once'})['value']即可 即抓取含有 name=\"once\"的input标签中的value对应的值。 于是构建postData,然后POST。 怎么显示登录成功呢？这里通过访问 http://www.v2ex.com/settings 即可，因为这个网址没有登录是看不了的： 经过上面的分析，写出源代码: import requests from bs4 import BeautifulSoup url = \"http://www.v2ex.com/signin\" UA = \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.13 Safari/537.36\" header = { \"User-Agent\" : UA, \"Referer\": \"http://www.v2ex.com/signin\" } v2ex_session = requests.Session() f = v2ex_session.get(url,headers=header) soup = BeautifulSoup(f.content,\"html.parser\") once = soup.find('input',{'name':'once'})['value'] print(once) postData = { 'u': 'whatbeg', 'p': '*****', 'once': once, 'next': '/' } v2ex_session.post(url, data = postData, headers = header) f = v2ex_session.get('http://www.v2ex.com/settings',headers=header) print(f.content.decode()) 然后运行发现成功登录： 拉勾网数据爬取 拉钩网的 cookies 是动态变化的，传入的时候不要写固定，用 requests.session() 来保存动态的 cookies。 import requests headers = { 'Accept': 'application/json, text/javascript, */*; q=0.01', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'Content-Length': '25', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Host': 'www.lagou.com', 'Origin': 'https://www.lagou.com', 'Referer': 'https://www.lagou.com/jobs/list_Python?labelWords=&fromSearch=true&suginput=', 'Sec-Fetch-Mode': 'cors', 'Sec-Fetch-Site': 'same-origin', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36', 'X-Anit-Forge-Code': '0', 'X-Anit-Forge-Token': 'None', 'X-Requested-With': 'XMLHttpRequest' } base_url2 = 'https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false' base_url = 'https://www.lagou.com/jobs/positionAjax.json?' page = 1 params = { 'city': '北京', 'needAddtionalResult': 'false' } form_data = { 'first': 'false', 'pn': '3', 'kd': 'python' } session = requests.session() session.get(base_url2, headers={ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36' }) response = session.post(url=base_url , headers=headers , params=params , data=form_data , allow_redirects=False ) print(response.json()) 参考 Python模拟登录(一) requests.Session应用 Update time： 2020-08-13 "},"Chapter1/XPath语法和lxml模块.html":{"url":"Chapter1/XPath语法和lxml模块.html","title":"XPath语法和lxml模块","keywords":"","body":"XPath语法和lxml模块 什么是XPath？ xpath（XML Path Language）是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历。 XPath语法 选取节点： XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。 表达式 描述 示例 结果 nodename 选取此节点的所有子节点 bookstore 选取bookstore下所有的子节点 / 如果是在最前面，代表从根节点选取。否则选择某节点下的某个节点 /bookstore 选取根元素下所有的bookstore节点 // 从全局节点中选择节点，随便在哪个位置 //book 从全局节点中找到所有的book节点 @ 选取某个节点的属性 //book[@price] 选择所有拥有price属性的book节点 . 当前节点 ./a 选取当前节点下的a标签 谓语： 谓语用来查找某个特定的节点或者包含某个指定的值的节点，被嵌在方括号中。 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 通配符 *表示通配符 通配符 描述 示例 结果 * 匹配任意节点 /bookstore/* 选取bookstore下的所有子元素。 @* 匹配节点中的任何属性 //book[@*] 选取所有带有属性的book元素。 选取多个路径： 通过在路径表达式中使用 “|” 运算符，可以选取若干个路径。 //bookstore/book | //book/title # 选取所有book元素以及book元素下所有的title元素 lxml库 lxml 是 一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML/XML 数据。 lxml和正则一样，也是用 C 实现的，是一款高性能的 Python HTML/XML 解析器，我们可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。 基本使用： 解析HTML代码，并且在解析HTML代码的时候，如果HTML代码不规范，他会自动的进行补全。示例代码如下： # 使用 lxml 的 etree 库 from lxml import etree text = ''' first item second item third item fourth item fifth item # 注意，此处缺少一个 闭合标签 ''' #利用etree.HTML，将字符串解析为HTML文档 html = etree.HTML(text) # 按字符串序列化HTML文档 result = etree.tostring(html) print(result) 输入结果如下： first item second item third item fourth item fifth item 可以看到。lxml会自动修改HTML代码。例子中不仅补全了li标签，还添加了body，html标签。 从文件中读取html代码 除了直接使用字符串进行解析，lxml还支持从文件中读取内容。我们新建一个hello.html文件： first item second item third item fourth item fifth item 然后利用 etree.parse()方法来读取文件。示例代码如下： from lxml import etree # 读取外部文件 hello.html html = etree.parse('hello.html') result = etree.tostring(html, pretty_print=True) print(result) 在lxml中使用XPath语法： 读取文本解析节点 from lxml import etree text=''' 第一个 second item a属性 ''' html=etree.HTML(text) #初始化生成一个XPath解析对象 result=etree.tostring(html,encoding='utf-8') #解析对象输出代码 print(type(html)) print(type(result)) print(result.decode('utf-8')) #etree会修复HTML文本节点 第一个 second item a属性 读取HTML文件进行解析 from lxml import etree html=etree.parse('test.html',etree.HTMLParser()) #指定解析器HTMLParser会根据文件修复HTML文件中缺失的如声明信息 result=etree.tostring(html) #解析成字节 #result=etree.tostringlist(html) #解析成列表 print(type(html)) print(type(result)) print(result) # b'\\n \\n \\n first item \\n second item \\n third item \\n fourth item \\n fifth item \\n \\n \\n' 获取所有节点 返回一个列表每个元素都是Element类型，所有节点都包含在其中 from lxml import etree html=etree.parse('test',etree.HTMLParser()) result=html.xpath('//*') #//代表获取子孙节点，*代表获取所有 print(type(html)) print(type(result)) print(result) # [, , , , , , , , , , , , , ] 如要获取li节点，可以使用//后面加上节点名称，然后调用xpath()方法 html.xpath('//li') #获取所有子孙节点的li节点 获取子节点 通过/或者//即可查找元素的子节点或者子孙节点，如果想选择li节点的所有直接a节点，可以这样使用 result=html.xpath('//li/a') #通过追加/a选择所有li节点的所有直接a节点， # 因为//li用于选中所有li节点，/a用于选中li节点的所有直接子节点a 属性匹配 在选取的时候，我们还可以用@符号进行属性过滤。比如，这里如果要选取class为item-1的li节点，可以这样实现: from lxml import etree from lxml.etree import HTMLParser text=''' 第一个 second item ''' html=etree.HTML(text,etree.HTMLParser()) result=html.xpath('//li[@class=\"item-1\"]') print(result) 文本获取 我们用XPath中的text()方法获取节点中的文本 from lxml import etree text=''' 第一个 second item ''' html=etree.HTML(text,etree.HTMLParser()) result=html.xpath('//li[@class=\"item-1\"]/a/text()') #获取a节点下的内容 result1=html.xpath('//li[@class=\"item-1\"]//text()') #获取li下所有子孙节点的内容 print(result) print(result1) 属性获取 使用@符号即可获取节点的属性，如下：获取所有li节点下所有a节点的href属性 result=html.xpath('//li/a/@href') #获取a的href属性 result=html.xpath('//li//@href') #获取所有li子孙节点的href属性 属性多值匹配 如果某个属性的值有多个时，我们可以使用contains()函数来获取 from lxml import etree text1=''' 第一个 second item ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\"aaa\"]/a/text()') result1=html.xpath('//li[contains(@class,\"aaa\")]/a/text()') print(result) print(result1) #通过第一种方法没有取到值，通过contains（）就能精确匹配到节点了 [] ['第一个'] 多属性匹配 另外我们还可能遇到一种情况，那就是根据多个属性确定一个节点，这时就需要同时匹配多个属性，此时可用运用and运算符来连接使用： from lxml import etree text1=''' 第一个 second item ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\"aaa\" and @name=\"fore\"]/a/text()') result1=html.xpath('//li[contains(@class,\"aaa\") and @name=\"fore\"]/a/text()') print(result) print(result1) # ['second item'] ['second item'] 按序选择 有时候，我们在选择的时候某些属性可能同时匹配多个节点，但我们只想要其中的某个节点，如第二个节点或者最后一个节点，这时可以利用中括号引入索引的方法获取特定次序的节点： from lxml import etree text1=''' 第一个 第二个 第三个 第四个 ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[contains(@class,\"aaa\")]/a/text()') #获取所有li节点下a节点的内容 result1=html.xpath('//li[1][contains(@class,\"aaa\")]/a/text()') #获取第一个 result2=html.xpath('//li[last()][contains(@class,\"aaa\")]/a/text()') #获取最后一个 result3=html.xpath('//li[position()>2 and position() 这里使用了last()、position()函数，在XPath中，提供了100多个函数，包括存取、数值、字符串、逻辑、节点、序列等处理功能，它们的具体作用可参考：http://www.w3school.com.cn/xpath/xpath_functions.asp lxml结合xpath注意事项： 使用xpath语法。应该使用Element.xpath方法。来执行xpath的选择。示例代码如下 trs = html.xpath(\"//tr[position()>1]\") xpath函数返回来的永远是一个列表。 获取某个标签的属性： href = html.xpath(\"//a/@href\") # 获取a标签的href属性对应的值 若再某个标签下 也可以直接用get()函数 html = etree.HTML(text) imgs = html.xpath(\"//div[@class='page-content text-center']//img[@class!='gif']\") for img in imgs: img_url = img.get('data-original') alt = img.get('alt') 获取文本，是通过xpath中的text()函数。示例代码如下： address = tr.xpath(\"./td[4]/text()\")[0] 在某个标签下，再执行xpath函数，获取这个标签下的子孙元素，那么应该在斜杠之前加一个点. ，代表是在当前元素下获取。示例代码如下： address = tr.xpath(\"./td[4]/text()\")[0] # address = tr.xpath(\"./td[4]//text()\")[0] 子孙标签的所有文本 需要注意的知识点： / 和// 的区别：/ 代表只获取直接子节点。// 获取子孙节点。一般// 用得比较多。当然也要视情况而定。 contains：有时候某个属性中包含了多个值，那么可以使用contains函数。示例代码如下：//div[contains(@class,'job_detail')] 谓词中的下标是从1开始的，不是从0开始的。 练习 #encoding: utf-8 from lxml import etree # 1. 获取所有tr标签 # 2. 获取第2个tr标签 # 3. 获取所有class等于even的tr标签 # 4. 获取所有a标签的href属性 # 5. 获取所有的职位信息（纯文本） parser = etree.HTMLParser(encoding='utf-8') html = etree.parse(\"tencent.html\",parser=parser) # 1. 获取所有tr标签 # //tr # xpath函数返回的是一个列表 # trs = html.xpath(\"//tr\") # for tr in trs: # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 2. 获取第2个tr标签 # tr = html.xpath(\"//tr[2]\")[0] # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 3. 获取所有class等于even的tr标签 # trs = html.xpath(\"//tr[@class='even']\") # for tr in trs: # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 4. 获取所有a标签的href属性 # aList = html.xpath(\"//a/@href\") # for a in aList: # print(\"http://hr.tencent.com/\"+a) # 5. 获取所有的职位信息（纯文本） trs = html.xpath(\"//tr[position()>1]\") positions = [] for tr in trs: # 在某个标签下，再执行xpath函数，获取这个标签下的子孙元素 # 那么应该在//之前加一个点，代表是在当前元素下获取 href = tr.xpath(\".//a/@href\")[0] fullurl = 'http://hr.tencent.com/' + href title = tr.xpath(\"./td[1]//text()\")[0] category = tr.xpath(\"./td[2]/text()\")[0] nums = tr.xpath(\"./td[3]/text()\")[0] address = tr.xpath(\"./td[4]/text()\")[0] pubtime = tr.xpath(\"./td[5]/text()\")[0] position = { 'url': fullurl, 'title': title, 'category': category, 'nums': nums, 'address': address, 'pubtime': pubtime } positions.append(position) print(positions) 案例应用：抓取TIOBE指数前20名排行开发语言 #!/usr/bin/env python #coding:utf-8 import requests from requests.exceptions import RequestException from lxml import etree from lxml.etree import ParseError import json def one_to_page(html): headers={ 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36' } try: response=requests.get(html,headers=headers) body=response.text #获取网页内容 except RequestException as e: print('request is error!',e) try: html=etree.HTML(body,etree.HTMLParser()) #解析HTML文本内容 result=html.xpath('//table[contains(@class,\"table-top20\")]/tbody/tr//text()') #获取列表数据 pos = 0 for i in range(20): if i == 0: yield result[i:5] else: yield result[pos:pos+5] #返回排名生成器数据 pos+=5 except ParseError as e: print(e.position) def write_file(data): #将数据重新组合成字典写入文件并输出 for i in data: sul={ '2018年6月排行':i[0], '2017年6排行':i[1], '开发语言':i[2], '评级':i[3], '变化率':i[4] } with open('test.txt','a',encoding='utf-8') as f: f.write(json.dumps(sul,ensure_ascii=False) + '\\n') #必须格式化数据 f.close() print(sul) return None def main(): url='https://www.tiobe.com/tiobe-index/' data=one_to_page(url) revaule=write_file(data) if revaule == None: print('ok') if __name__ == '__main__': main() # {'2018年6月排行': '1', '2017年6排行': '1', '开发语言': 'Java', '评级': '15.368%', '变化率': '+0.88%'} {'2018年6月排行': '2', '2017年6排行': '2', '开发语言': 'C', '评级': '14.936%', '变化率': '+8.09%'} {'2018年6月排行': '3', '2017年6排行': '3', '开发语言': 'C++', '评级': '8.337%', '变化率': '+2.61%'} {'2018年6月排行': '4', '2017年6排行': '4', '开发语言': 'Python', '评级': '5.761%', '变化率': '+1.43%'} {'2018年6月排行': '5', '2017年6排行': '5', '开发语言': 'C#', '评级': '4.314%', '变化率': '+0.78%'} {'2018年6月排行': '6', '2017年6排行': '6', '开发语言': 'Visual Basic .NET', '评级': '3.762%', '变化率': '+0.65%'} {'2018年6月排行': '7', '2017年6排行': '8', '开发语言': 'PHP', '评级': '2.881%', '变化率': '+0.11%'} {'2018年6月排行': '8', '2017年6排行': '7', '开发语言': 'JavaScript', '评级': '2.495%', '变化率': '-0.53%'} {'2018年6月排行': '9', '2017年6排行': '-', '开发语言': 'SQL', '评级': '2.339%', '变化率': '+2.34%'} {'2018年6月排行': '10', '2017年6排行': '14', '开发语言': 'R', '评级': '1.452%', '变化率': '-0.70%'} {'2018年6月排行': '11', '2017年6排行': '11', '开发语言': 'Ruby', '评级': '1.253%', '变化率': '-0.97%'} {'2018年6月排行': '12', '2017年6排行': '18', '开发语言': 'Objective-C', '评级': '1.181%', '变化率': '-0.78%'} {'2018年6月排行': '13', '2017年6排行': '16', '开发语言': 'Visual Basic', '评级': '1.154%', '变化率': '-0.86%'} {'2018年6月排行': '14', '2017年6排行': '9', '开发语言': 'Perl', '评级': '1.147%', '变化率': '-1.16%'} {'2018年6月排行': '15', '2017年6排行': '12', '开发语言': 'Swift', '评级': '1.145%', '变化率': '-1.06%'} {'2018年6月排行': '16', '2017年6排行': '10', '开发语言': 'Assembly language', '评级': '0.915%', '变化率': '-1.34%'} {'2018年6月排行': '17', '2017年6排行': '17', '开发语言': 'MATLAB', '评级': '0.894%', '变化率': '-1.10%'} {'2018年6月排行': '18', '2017年6排行': '15', '开发语言': 'Go', '评级': '0.879%', '变化率': '-1.17%'} {'2018年6月排行': '19', '2017年6排行': '13', '开发语言': 'Delphi/Object Pascal', '评级': '0.875%', '变化率': '-1.28%'} {'2018年6月排行': '20', '2017年6排行': '20', '开发语言': 'PL/SQL', '评级': '0.848%', '变化率': '-0.72%'} 使用requests和xpath爬取电影天堂 import requests from lxml import etree BASE_DOMAIN = 'http://www.dytt8.net' HEADERS = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36', 'Referer': 'http://www.dytt8.net/html/gndy/dyzz/list_23_2.html' } def spider(): url = 'http://www.dytt8.net/html/gndy/dyzz/list_23_1.html' resp = requests.get(url,headers=HEADERS) # resp.content：经过编码后的字符串 # resp.text：没有经过编码，也就是unicode字符串 # text：相当于是网页中的源代码了 text = resp.content.decode('gbk') # tree：经过lxml解析后的一个对象，以后使用这个对象的xpath方法，就可以 # 提取一些想要的数据了 tree = etree.HTML(text) # xpath/beautifulsou4 all_a = tree.xpath(\"//div[@class='co_content8']//a\") for a in all_a: title = a.xpath(\"text()\")[0] href = a.xpath(\"@href\")[0] if href.startswith('/'): detail_url = BASE_DOMAIN + href crawl_detail(detail_url) break def crawl_detail(url): resp = requests.get(url,headers=HEADERS) text = resp.content.decode('gbk') tree = etree.HTML(text) create_time = tree.xpath(\"//div[@class='co_content8']/ul/text()\")[0].strip() imgs = tree.xpath(\"//div[@id='Zoom']//img/@src\") # 电影海报 cover = imgs[0] # 电影截图 screenshoot = imgs[1] # 获取span标签下所有的文本 infos = tree.xpath(\"//div[@id='Zoom']//text()\") for index,info in enumerate(infos): if info.startswith(\"◎年　　代\"): year = info.replace(\"◎年　　代\",\"\").strip() if info.startswith(\"◎豆瓣评分\"): douban_rating = info.replace(\"◎豆瓣评分\",'').strip() print(douban_rating) if info.startswith(\"◎主　　演\"): # 从当前位置，一直往下面遍历 actors = [info] for x in range(index+1,len(infos)): actor = infos[x] if actor.startswith(\"◎\"): break actors.append(actor.strip()) print(\",\".join(actors)) if __name__ == '__main__': spider() 参考 XPath语法详解 XPath-语法大全 XPath的更多用法参考：http://www.w3school.com.cn/xpath/index.asp python lxml库的更多用法参考：http://lxml.de/ python3解析库lxml Update time： 2020-05-26 "},"Chapter1/正则表达式匹配.html":{"url":"Chapter1/正则表达式匹配.html","title":"正则表达式匹配","keywords":"","body":"正则表达式匹配 正则表达式 正则表达式（Regular Expression，简称Regex或RE）又称为正规表示法或常规表示法，常常用来检索、替换那些符合某个模式的文本，它首先设定好了一些特殊的字及字符组合，通过组合的“规则字符串”来对表达式进行过滤，从而获取或匹配我们想要的特定内容。它具有灵活、逻辑性和功能性非常的强，能迅速地通过表达式从字符串中找到所需信息的优点。 re模块 Python通过 re 模块提供对正则表达式的支持，使用正则表达式之前需要导入该库。 import re 其基本步骤是先将正则表达式的字符串形式编译为Pattern实例，然后使用Pattern实例处理文本并获得一个匹配（Match）实例，再使用Match实例获得所需信息。常用的函数是 findall，原型如下： findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags]) 该函数表示搜索字符串string，以列表形式返回全部能匹配的子串。 其中参数re包括三个常见值： re.I(re.IGNORECASE)：忽略大小写（括号内是完整写法） re.M(re.MULTILINE)：允许多行模式 re.S(re.DOTALL)：支持点任意匹配模式 Pattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。Pattern不能直接实例化，必须使用re.compile()进行构造。 complie方法 re正则表达式模块包括一些常用的操作函数，比如complie()函数。其原型如下 compile(pattern[,flags] ) 该函数根据包含正则表达式的字符串创建模式对象，返回一个pattern对象。参数flags是匹配模式，可以使用按位或“|”表示同时生效，也可以在正则表达式字符串中指定。Pattern对象是不能直接实例化的，只能通过compile方法得到。 简单举个实例，使用正则表达式获取字符串中的数字内容，如下所示： >>> import re >>> string=\"A1.45，b5，6.45，8.82\" >>> regex = re.compile(r\"\\d+\\.?\\d*\") >>> print regex.findall(string) ['1.45', '5', '6.45', '8.82'] >>> match方法 match方法是从字符串的 pos 下标处起开始匹配 pattern，如果pattern结束时已经匹配，则返回一个Match对象；如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。该方法原型如下： match(string[, pos[, endpos]]) | re.match(pattern, string[, flags]) 参数string表示字符串；pos表示下标，pos和endpos的默认值分别为0和len(string)；参数flags用于编译pattern时指定匹配模式。 search方法 search方法用于查找字符串中可以匹配成功的子串。从字符串的pos下标处起尝试匹配pattern，如果pattern结束时仍可匹配，则返回一个Match对象；若无法匹配，则将pos加1后重新尝试匹配；直到pos=endpos时仍无法匹配则返回None。 函数原型如下： search(string[, pos[, endpos]]) | re.search(pattern, string[, flags]) 参数string表示字符串；pos表示下标，pos和endpos的默认值分别为0和len(string))；参数flags用于编译pattern时指定匹配模式。 group和groups方法 group([group1, …])方法用于获得一个或多个分组截获的字符串，当它指定多个参数时将以元组形式返回。groups([default])方法以元组形式返回全部分组截获的字符串，相当于调用group(1,2,…last)。default表示没有截获字符串的组以这个值替代，默认为None。 正则表达式抓取网络数据常见方法 HTML语言是采用标签对的形式来编写网站的，包括起始标签和结束标签，比如、、等。下面讲解抓取标签对之间的文本内容。 抓取title标签间的内容 首先爬取网页的标题，采用的正则表达式为(.*?)，爬取百度标题代码如下： # coding=utf-8 import re import urllib url = \"http://www.baidu.com/\" content = urllib.urlopen(url).read() title = re.findall(r'(.*?)', content) print title[0] # 百度一下，你就知道 抓取超链接标签间的内容 在HTML中，用于标识超链接，获取完整的超链接和超链接和之间的内容 # coding=utf-8 import re import urllib url = \"http://www.baidu.com/\" content = urllib.urlopen(url).read() #获取完整超链接 res = r\"\" urls = re.findall(res, content) for u in urls: print unicode(u,'utf-8') #获取超链接和之间内容 res = r'(.*?)' texts = re.findall(res, content, re.S|re.M) for t in texts: print unicode(t,'utf-8') 输出结果部分内容如下所示，这里如果直接输出print u或print t可能会乱码，需要调用函数unicode(u,'utf-8')进行转码 #获取完整超链接 新闻 hao123 地图 视频 ... #获取超链接和之间内容 新闻 hao123 地图 视频 ... 抓取tr\\td标签间的内容 网页中常用的布局包括table布局或div布局，其中table表格布局中常见的标签包括tr、th和td，表格行为tr（table row），表格数据为td（table data），表格表头th（table heading）。那么如何抓取这些标签之间的内容呢？下面代码是获取它们之间内容。 假设存在HTML代码如下所示： 表格 学号姓名 1001杨秀璋 1002严娜 则爬取对应值的Python代码如下： # coding=utf-8 import re import urllib content = urllib.urlopen(\"test.html\").read() #打开本地文件 #获取间内容 res = r'(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: print m \"\"\" texts: ['学号姓名', '1001杨秀璋', '1002严娜'] \"\"\" #获取间内容 for m in texts: res_th = r'(.*?)' m_th = re.findall(res_th, m, re.S|re.M) for t in m_th: print t #直接获取间内容 res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) \"\"\" texts: [('1001', '杨秀璋'), ('1002', '严娜')] \"\"\" for m in texts: print m[0],m[1] 输出结果如下，首先获取tr之间的内容，然后再在tr之间内容中获取和之间值，即“学号”、“姓名”，最后讲述直接获取两个之间的内容方法。 >>> 学号姓名 1001杨秀璋 1002严娜 学号 姓名 1001 杨秀璋 1002 严娜 >>> 抓取标签中的参数 抓取超链接标签的URL HTML超链接的基本格式为链接内容，现在需要获取其中的URL链接地址，方法如下： # coding=utf-8 import re content = ''' 新闻 hao123 地图 视频 ''' res = r\"(? 输出内容如下： >>> http://news.baidu.com http://www.hao123.com http://map.baidu.com http://v.baidu.com >>> (? 解释 如果表达式为“(? (?=) 解释 如果表达式为“(?=.com)”，意思是：一个字符串后面跟着“.com”才做匹配操作，并不使用任何目标字符串 抓取图片超链接标签的URL HTML插入图片使用标签的基本格式为，则需要获取图片URL链接地址的方法如下： content = '''''' urls = re.findall('src=\"(.*?)\"', content, re.I|re.S|re.M) print urls # ['http://www..csdn.net/eastmount.jpg'] 其中图片对应的超链接为http://www..csdn.net/eastmount.jpg，这些资源通常存储在服务器端，最后一个“/”后面的字段即为资源的名称，该图片名称为“eastmount.jpg”。那么如何获取URL中最后一个参数呢？ 获取URL中最后一个参数 通常在使用Python爬取图片过程中，会遇到图片对应的URL最后一个字段通常用于命名图片，如前面的“eastmount.jpg”，需要通过URL“/”后面的参数获取图片。 content = '''''' urls = 'http://www..csdn.net/eastmount.jpg' name = urls.split('/')[-1] print name # eastmount.jpg 该段代码表示采用字符“/”分割字符串，并且获取最后一个获取的值，即为图片名称。 字符串处理及替换 在使用正则表达式爬取网页文本时，通常需要调用find() 函数找到指定的位置，再进行进一步爬取，比如获取class属性为“infobox”的表格table，再进行定位爬取。 start = content.find(r'') #重点点位置 infobox = text[start:end] print infobox 同时爬取过程中可能会爬取到无关变量，此时需要对无关内容进行过滤，这里推荐使用replace函数和正则表达式进行处理。比如，爬取内容如下： # coding=utf-8 import re content = ''' 1001杨秀璋 1002颜 娜 1003Python ''' res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: print m[0],m[1] 输出如下所示： >>> 1001 杨秀璋 1002 颜 娜 1003 Python >>> 此时需要过滤多余字符串，如换行（）、空格（ ）、加粗（）。 过滤代码如下： # coding=utf-8 import re content = ''' 1001杨秀璋 1002颜 娜 1003Python ''' res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: value0 = m[0].replace('', '').replace(' ', '') value1 = m[1].replace('', '').replace(' ', '') if '' in value1: m_value = re.findall(r'(.*?)', value1, re.S|re.M) print value0, m_value[0] else: print value0, value1 采用replace将字符串“”或“' ”替换成空白，实现过滤，而加粗（）需要使用正则表达式过滤，输出结果如下： >>> 1001 杨秀璋 1002 颜娜 1003 Python >>> 参考 [python爬虫] 正则表达式使用技巧及爬取个人博客实例 Update time： 2020-05-26 "},"Chapter1/chardet识别文件的编码格式.html":{"url":"Chapter1/chardet识别文件的编码格式.html","title":"chardet识别文件的编码格式","keywords":"","body":"识别文件的编码格式 chardet库文档:https://chardet.readthedocs.io/en/latest/usage.html 模块介绍 Chardet：通用字符编码检测器 检测字符集范围： ASCII，UTF-8，UTF-16（2种变体），UTF-32（4种变体） Big5，GB2312，EUC-TW，HZ-GB-2312，ISO-2022-CN（繁体中文和简体中文） EUC-JP，SHIFT_JIS，CP932，ISO-2022-JP（日文） EUC-KR，ISO-2022-KR（韩文） KOI8-R，MacCyrillic，IBM855，IBM866，ISO-8859-5，windows-1251（西里尔文） ISO-8859-5，windows-1251（保加利亚语） ISO-8859-1，windows-1252（西欧语言） ISO-8859-7，windows-1253（希腊语） ISO-8859-8，windows-1255（视觉和逻辑希伯来语） TIS-620（泰国语）d'y 当python程序中某一个数据文件不知道编码时，可使用chardet第三方库来检测，代码如下（path中填对应文件路径即可) import chardet if __name__ == '__main__': path='***' f=open(path,'rb') data=f.read() print(chardet.detect(data)) # {'language': '', 'confidence': 0.73, 'encoding': 'Windows-1252'} detect函数只需要一个 非unicode字符串参数，返回一个字典。该字典包括判断到的编码格式及判断的置信度。 chardet.detect() 的返回值，为一个字典： {'language': '', 'confidence': 0.73, 'encoding': 'Windows-1252'} 得到文件的编码方式，可以才采用字典的方式 codedetect = chardet.detect(data)[\"encoding\"] #检测得到编码方式 将获取的内容进行解码 code = chardet.detect(response.content)[\"encoding\"] # 获取编码格式 response.encoding = code # 指定编码格式 return response.text Update time： 2020-05-26 "},"Chapter1/fake-useragent.html":{"url":"Chapter1/fake-useragent.html","title":"fake-useragent","keywords":"","body":"fake-useragent Python库(fake-useragent),可以随机生成各种UserAgent 安装 pip install fake-useragent 基本使用 from fake_useragent import UserAgent ua = UserAgent() from fake_useragent import UserAgent ua = UserAgent() ua.ie # Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US); ua.msie # Mozilla/5.0 (compatible; MSIE 10.0; Macintosh; Intel Mac OS X 10_7_3; Trident/6.0)' ua['Internet Explorer'] # Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.2; SV1; .NET CLR 3.3.69573; WOW64; en-US) ua.opera # Opera/9.80 (X11; Linux i686; U; ru) Presto/2.8.131 Version/11.11 ua.chrome # Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2' ua.google # Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/537.13 (KHTML, like Gecko) Chrome/24.0.1290.1 Safari/537.13 ua['google chrome'] # Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11 ua.firefox # Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0.1) Gecko/20121011 Firefox/16.0.1 ua.ff # Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:15.0) Gecko/20100101 Firefox/15.0.1 ua.safari # Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25 # and the best one, random via real world browser usage statistic ua.random 注意: fake-useragent 将收集到的数据缓存到temp文件夹, 例如 /tmp, 更新数据: from fake_useragent import UserAgent ua = UserAgent() ua.update() 有时候会因为网络或者其他问题,出现异常(fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached), 可以禁用服务器缓存 from fake_useragent import UserAgent ua = UserAgent(use_cache_server=False) 可以自己添加本地数据文件 import fake_useragent # I am STRONGLY!!! recommend to use version suffix location = '/home/user/fake_useragent%s.json' % fake_useragent.VERSION ua = fake_useragent.UserAgent(path=location) ua.random 参考 fake-useragent 0.1.11 Update time： 2020-05-26 "},"Selenium/":{"url":"Selenium/","title":"Selenium","keywords":"","body":"Selenium Update time： 2020-08-12 "},"Selenium/基本使用.html":{"url":"Selenium/基本使用.html","title":"基本使用","keywords":"","body":"基本使用 AJAX动态网页数据抓取、Selenium使用 什么是AJAX AJAX（Asynchronouse JavaScript And XML）异步JavaScript和XML。过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页（不使用Ajax）如果需要更新内容，必须重载整个网页页面。因为传统的在传输数据格式方面，使用的是XML语法。因此叫做AJAX，其实现在数据交互基本上都是使用JSON。使用AJAX加载的数据，即使使用了JS，将数据渲染到了浏览器中，在右键->查看网页源代码还是不能看到通过ajax加载的数据，只能看到使用这个url加载的html代码。 获取ajax数据的方式 直接分析ajax调用的接口。然后通过代码请求这个接口。 使用Selenium+chromedriver模拟浏览器行为获取数据。 接口分析 豆瓣电影爬取 https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&type=24& interval_id=100:90&action= 打开页面，电影页面没有分页显示按钮，当鼠标向下滚动时，会加载新的电影， 查看网页，查看Request URL:分析 https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start=0&limit=20 URL中的 start 参数是动态变化的 测试，能获取‘一页’的电影 import requests url = \"https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start=20&limit=20\" res = requests.get(url) print(res.text) 测试，获取所有电影 import requests base_url = \"https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start={}&limit=20\" #可以适当的增加 limit= 的值 提高效率 i = 0 while True: print(i) url=base_url.format(i*20) res=requests.get(url) info = res.text print(len(info)) if len(info) == 0: break i += 1 Selenium+chromedriver获取动态数据 Selenium 相当于是一个机器人。可以模拟人类在浏览器上的一些行为，自动处理浏览器上的一些行为，比如点击，填充数据，删除cookie等。chromedriver是一个驱动Chrome浏览器的驱动程序，使用他才可以驱动浏览器。当然针对不同的浏览器有不同的driver。以下列出了不同浏览器及其对应的driver： Chrome Firefox Edge Safari 安装Selenium和chromedriver 安装Selenium：Selenium有很多语言的版本，有java、ruby、python等。我们下载python版本的就可以了 pip install selenium 安装chromedriver：下载完成后，放到不需要权限的纯英文目录下就可以了。 快速入门 现在以一个简单的获取百度首页的例子来讲下Selenium和chromedriver如何快速入门： from selenium import webdriver # chromedriver的绝对路径 driver_path = r'D:\\ProgramApp\\chromedriver\\chromedriver.exe' # 初始化一个driver，并且指定chromedriver的路径 driver = webdriver.Chrome(executable_path=driver_path) # 或者 driver = webdriver.Chrome() # 请求网页 driver.get(\"https://www.baidu.com/\") # 通过page_source获取网页代码（浏览器渲染完后的代码） print(driver.page_source) selenium常用操作 更多教程请参考 定位元素方法 Selenium提供了一下方法来定义一个页面中的元素： find_element_by_id 通过ID查找元素 当你知道一个元素的 id 时，你可以使用本方法。在该策略下，页面中第一个该 id 元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: 可以这样查找表单(form)元素: login_form = driver.find_element_by_id('loginForm') find_element_by_name通过Name查找元素 当你知道一个元素的 name 时，你可以使用本方法。在该策略下，页面中第一个该 name 元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: name属性为 username & password 的元素可以像下面这样查找: username = driver.find_element_by_name('username') password = driver.find_element_by_name('password') 这会得到 “Login” 按钮，因为他在 “Clear” 按钮之前: continue = driver.find_element_by_name('continue') find_element_by_xpath 通过XPath查找元素 XPath是XML文档中查找结点的语法。因为HTML文档也可以被转换成XML(XHTML)文档， Selenium的用户可以利用这种强大的语言在web应用中查找元素。 XPath扩展了（当然也支持）这种通过id或name属性获取元素的简单方式，同时也开辟了各种新的可能性， 例如获取页面上的第三个复选框。 使用XPath的主要原因之一就是当你想获取一个既没有id属性也没有name属性的元素时， 你可以通过XPath使用元素的绝对位置来获取他（这是不推荐的），或相对于有一个id或name属性的元素 （理论上的父元素）的来获取你想要的元素。XPath定位器也可以通过非id和name属性查找元素。 绝对的XPath是所有元素都从根元素的位置（HTML）开始定位，只要应用中有轻微的调整，会就导致你的定位失败。 但是通过就近的包含id或者name属性的元素出发定位你的元素，这样相对关系就很靠谱， 因为这种位置关系很少改变，所以可以使你的测试更加强大。 作为示例，页面元素如下所示: 可以这样查找表单(form)元素: login_form = driver.find_element_by_xpath(\"/html/body/form[1]\") login_form = driver.find_element_by_xpath(\"//form[1]\") login_form = driver.find_element_by_xpath(\"//form[@id='loginForm']\") 绝对定位 (页面结构轻微调整就会被破坏) HTML页面中的第一个form元素 包含 id 属性并且其值为 loginForm 的form元素 username元素可以如下获取: username = driver.find_element_by_xpath(\"//form[input/@name='username']\") username = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[1]\") username = driver.find_element_by_xpath(\"//input[@name='username']\") “Clear” 按钮可以如下获取: clear_button = driver.find_element_by_xpath(\"//input[@name='continue'][@type='button']\") clear_button = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[4]\") 这些实例都是一些举出用法, 为了学习更多有用的东西，下面这些参考资料推荐给你: W3Schools XPath Tutorial W3C XPath Recommendation XPath Tutorial - with interactive examples. 还有一些非常有用的插件，可以协助发现元素的XPath: XPath Checker - suggests XPath and can be used to test XPath results. Firebug - XPath suggestions are just one of the many powerful features of this very useful add-on. XPath Helper - for Google Chrome find_element_by_link_text 通过链接文本获取超链接 当你知道在一个锚标签中使用的链接文本时使用这个。 在该策略下，页面中第一个匹配链接内容锚标签 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Are you sure you want to do this? Continue Cancel continue.html 超链接可以被这样查找到: continue_link = driver.find_element_by_link_text('Continue') continue_link = driver.find_element_by_partial_link_text('Conti') find_element_by_partial_link_text find_element_by_tag_name 通过标签名查找元素 当你向通过标签名查找元素时使用这个。 在该策略下，页面中第一个匹配该标签名的元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Welcome Site content goes here. h1 元素可以如下查找: heading1 = driver.find_element_by_tag_name('h1') find_element_by_class_name 当你向通过class name查找元素时使用这个。 在该策略下，页面中第一个匹配该class属性的元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Site content goes here. p 元素可以如下查找: content = driver.find_element_by_class_name('content') find_element_by_css_selector 当你向通过CSS选择器查找元素时使用这个。 在该策略下，页面中第一个匹配该CSS 选择器的元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Site content goes here. p 元素可以如下查找: content = driver.find_element_by_css_selector('p.content') 下面是查找多个元素（这些方法将返回一个列表） find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 常用方法是通过xpath相对路径进行定位, 例如 定位username元素的方法如下： username = driver.find_element_by_xpath(\"//form[input/@name='username']\") username = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[1]\") username = driver.find_element_by_xpath(\"//input[@name='username']\") [1] 第一个form元素通过一个input子元素，name属性和值为username实现 [2] 通过id=loginForm值的form元素找到第一个input子元素 [3] 属性名为name且值为username的第一个input元素 操作元素方法 通常所有的操作与页面交互都将通过WebElement接口，常见的操作元素方法如下： clear 清除元素的内容 send_keys 模拟按键输入 click 点击元素 submit 提交表单 举例自动访问FireFox浏览器自动登录163邮箱。 from selenium import webdriver from selenium.webdriver.common.keys import Keys import time # Login 163 email driver = webdriver.Firefox() driver.get(\"http://mail.163.com/\") elem_user = driver.find_element_by_name(\"username\") elem_user.clear() elem_user.send_keys(\"15201615157\") elem_pwd = driver.find_element_by_name(\"password\") elem_pwd.clear() elem_pwd.send_keys(\"******\") elem_pwd.send_keys(Keys.RETURN) #driver.find_element_by_id(\"loginBtn\").click() #driver.find_element_by_id(\"loginBtn\").submit() time.sleep(5) assert \"baidu\" in driver.title driver.close() driver.quit() 首先通过name定位用户名和密码，再调用方法clear()清除输入框默认内容，如“请输入密码”等提示，通过send_keys(\"**\")输入正确的用户名和密码，最后通过click()点击登录按钮或send_keys(Keys.RETURN)相当于回车登录，submit()提交表单。 PS：如果需要输入中文，防止编码错误使用send_keys(u\"中文用户名\")。 WebElement接口获取值 通过WebElement接口可以获取常用的值，这些值同样非常重要。 size 获取元素的尺寸 text 获取元素的文本 get_attribute(name) 获取属性值 location 获取元素坐标，先找到要获取的元素，再调用该方法 page_source 返回页面源码 driver.title 返回页面标题 current_url 获取当前页面的URL is_displayed() 设置该元素是否可见 is_enabled() 判断元素是否被使用 is_selected() 判断元素是否被选中 tag_name 返回元素的tagName from selenium import webdriver from selenium.webdriver.common.keys import Keys import time driver = webdriver.PhantomJS(executable_path=\"G:\\phantomjs-1.9.1-windows\\phantomjs.exe\") driver.get(\"http://www.baidu.com/\") size = driver.find_element_by_name(\"wd\").size print size #尺寸: {'width': 500, 'height': 22} news = driver.find_element_by_xpath(\"//div[@id='u1']/a[1]\").text print news #文本: 新闻 href = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('href') name = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('name') print href,name #属性值: http://www.hao123.com/ tj_trhao123 location = driver.find_element_by_xpath(\"//div[@id='u1']/a[3]\").location print location #坐标: {'y': 19, 'x': 498} print driver.current_url #当前链接: https://www.baidu.com/ print driver.title #标题: 百度一下， 你就知道 result = location = driver.find_element_by_id(\"su\").is_displayed() print result #是否可见: True 鼠标操作 在现实的自动化测试中关于鼠标的操作不仅仅是click()单击操作，还有很多包含在ActionChains类中的操作。如下： context_click(elem) 右击鼠标点击元素elem，另存为等行为 double_click(elem) 双击鼠标点击元素elem，地图web可实现放大功能 drag_and_drop(source,target) 拖动鼠标，源元素按下左键移动至目标元素释放 move_to_element(elem) 鼠标移动到一个元素上 click_and_hold(elem) 按下鼠标左键在一个元素上 perform() 在通过调用该函数执行ActionChains中存储行为 关闭浏览器 driver.close()：关闭当前页面。 driver.quit()：退出整个浏览器。 方法 区别 close 关闭当前的浏览器窗口 quit 不仅关闭窗口，还会彻底的退出webdriver，释放与driver server之间的连接 页面等待 现在的网页越来越多采用了 Ajax 技术，这样程序便不能确定何时某个元素完全加载出来了。如果实际页面等待时间过长导致某个dom元素还没出来，但是你的代码直接使用了这个WebElement，那么就会抛出NullPointer的异常。为了解决这个问题。所以 Selenium 提供了两种等待方式：一种是隐式等待、一种是显式等待。 隐式等待：调用driver.implicitly_wait。那么在获取不可用的元素之前，会先等待10秒中的时间。示例代码如下： driver = webdriver.Chrome(executable_path=driver_path) driver.implicitly_wait(10) # 请求网页 driver.get(\"https://www.douban.com/\") selenium_通过selenium控制浏览器滚动条 目的：通过selenium控制浏览器滚动条 原理：通过 driver.execute_script()执行js代码，达到目的 driver.execute_script(\"window.scrollBy(0,1000)\") 语法：scrollBy(x,y) 参数 描述 x 必需。向右滚动的像素值。 y 必需。向下滚动的像素值。 或者使用 js=\"var q=document.documentElement.scrollTop=10000\" driver.execute_script(js) 例如在京东的某个页面，当鼠标向下滚动的时候才会把整个页面的商品加载出来，利用 driver.execute_script() 可以设置某个阈值将页面一次滑倒最底端 from selenium import webdriver from lxml import etree from time import sleep url = 'https://search.jd.com/Search?keyword=mac&enc=utf-8&wq=mac&pvid=9862d03c24e741c6a58079d004f5aabf' chrome = webdriver.Chrome() chrome.get(url) js = 'document.documentElement.scrollTop=100000' chrome.execute_script(js) sleep(3) html = chrome.page_source e = etree.HTML(html) prices = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-price\"]/strong/i/text()') names = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-name p-name-type-2\"]/a/em') print(len(names)) for name, price in zip(names, prices): print(name.xpath('string(.)'), \":\", price) chrome.quit() selenium操作无界面chrome浏览器 from selenium import webdriver from selenium.webdriver.chrome.options import Options req_url = \"https://www.baidu.com\" chrome_options=Options() #设置chrome浏览器无界面模式 chrome_options.add_argument('--headless') browser = webdriver.Chrome(chrome_options=chrome_options) # 开始请求 browser.get(req_url) #打印页面源代码 print(browser.page_source) #关闭浏览器 browser.close() #关闭chreomedriver进程 browser.quit() selenium爬取局部动态刷新网站（URL始终固定） 测试，虎牙直播，当进入某个直播分类的时候，点击不同的分页，URL不发生变化，可以通过selenium的click()事件，实现翻页的情况， from selenium import webdriver from lxml import etree import time import requests driver_path=r'D:\\chromedriver_win32\\chromedriver.exe' driver = webdriver.Chrome(executable_path=driver_path) content=[] url = 'https://www.huya.com/g/wzry' driver.get(url) driver.implicitly_wait(1) m=1 def page(): global m global content while m 参考 [python爬虫] Selenium常见元素定位方法和操作的学习介绍 Selenium-Python中文文档 Update time： 2020-08-13 "},"Selenium/查找元素.html":{"url":"Selenium/查找元素.html","title":"查找元素","keywords":"","body":"查找元素 定位元素方法 Selenium提供了一下方法来定义一个页面中的元素： 在一个页面中有很多不同的策略可以定位一个元素。在你的项目中， 你可以选择最合适的方法去查找元素。Selenium提供了下列的方法给你: find_element_by_id find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 一次查找多个元素 (这些方法会返回一个list列表): find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 通过ID查找元素 find_element_by_id 当你知道一个元素的 id 时，你可以使用本方法。在该策略下，页面中第一个该 id 元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: 可以这样查找表单(form)元素: login_form = driver.find_element_by_id('loginForm') 通过Name查找元素 find_element_by_name 当你知道一个元素的 name 时，你可以使用本方法。在该策略下，页面中第一个该 name 元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: name属性为 username & password 的元素可以像下面这样查找: username = driver.find_element_by_name('username') password = driver.find_element_by_name('password') 这会得到 “Login” 按钮，因为他在 “Clear” 按钮之前: continue = driver.find_element_by_name('continue') 通过XPath查找元素 find_element_by_xpath XPath是XML文档中查找结点的语法。因为HTML文档也可以被转换成XML(XHTML)文档， Selenium的用户可以利用这种强大的语言在web应用中查找元素。 XPath扩展了（当然也支持）这种通过id或name属性获取元素的简单方式，同时也开辟了各种新的可能性， 例如获取页面上的第三个复选框。 使用XPath的主要原因之一就是当你想获取一个既没有id属性也没有name属性的元素时， 你可以通过XPath使用元素的绝对位置来获取他（这是不推荐的），或相对于有一个id或name属性的元素 （理论上的父元素）的来获取你想要的元素。XPath定位器也可以通过非id和name属性查找元素。 绝对的XPath是所有元素都从根元素的位置（HTML）开始定位，只要应用中有轻微的调整，会就导致你的定位失败。 但是通过就近的包含id或者name属性的元素出发定位你的元素，这样相对关系就很靠谱， 因为这种位置关系很少改变，所以可以使你的测试更加强大。 作为示例，页面元素如下所示: 可以这样查找表单(form)元素: login_form = driver.find_element_by_xpath(\"/html/body/form[1]\") login_form = driver.find_element_by_xpath(\"//form[1]\") login_form = driver.find_element_by_xpath(\"//form[@id='loginForm']\") 绝对定位 (页面结构轻微调整就会被破坏) HTML页面中的第一个form元素 包含 id 属性并且其值为 loginForm 的form元素 username元素可以如下获取: username = driver.find_element_by_xpath(\"//form[input/@name='username']\") username = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[1]\") username = driver.find_element_by_xpath(\"//input[@name='username']\") “Clear” 按钮可以如下获取: clear_button = driver.find_element_by_xpath(\"//input[@name='continue'][@type='button']\") clear_button = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[4]\") 这些实例都是一些举出用法, 为了学习更多有用的东西，下面这些参考资料推荐给你: W3Schools XPath Tutorial W3C XPath Recommendation XPath Tutorial - with interactive examples. 还有一些非常有用的插件，可以协助发现元素的XPath: XPath Checker - suggests XPath and can be used to test XPath results. Firebug - XPath suggestions are just one of the many powerful features of this very useful add-on. XPath Helper - for Google Chrome 通过链接文本获取超链接 find_element_by_link_text 当你知道在一个锚标签中使用的链接文本时使用这个。 在该策略下，页面中第一个匹配链接内容锚标签 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Are you sure you want to do this? Continue Cancel continue.html 超链接可以被这样查找到: continue_link = driver.find_element_by_link_text('Continue') continue_link = driver.find_element_by_partial_link_text('Conti') find_element_by_partial_link_text 通过标签名查找元素 find_element_by_tag_name 当你向通过标签名查找元素时使用这个。 在该策略下，页面中第一个匹配该标签名的元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Welcome Site content goes here. h1 元素可以如下查找: heading1 = driver.find_element_by_tag_name('h1') 通过Class name 定位元素 find_element_by_class_name 当你向通过class name查找元素时使用这个。 在该策略下，页面中第一个匹配该class属性的元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Site content goes here. p 元素可以如下查找: content = driver.find_element_by_class_name('content') 通过CSS选择器查找元素 find_element_by_css_selector 当你向通过CSS选择器查找元素时使用这个。 在该策略下，页面中第一个匹配该CSS 选择器的元素 会被匹配并返回。如果找不到任何元素，会抛出 NoSuchElementException 异常。 作为示例，页面元素如下所示: Site content goes here. p 元素可以如下查找: content = driver.find_element_by_css_selector('p.content') 下面是查找多个元素（这些方法将返回一个列表） find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 常用方法是通过xpath相对路径进行定位, 例如 定位username元素的方法如下： username = driver.find_element_by_xpath(\"//form[input/@name='username']\") username = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[1]\") username = driver.find_element_by_xpath(\"//input[@name='username']\") [1] 第一个form元素通过一个input子元素，name属性和值为username实现 [2] 通过id=loginForm值的form元素找到第一个input子元素 [3] 属性名为name且值为username的第一个input元素 参考 Selenium with Python中文翻译文档 : 查找元素 Update time： 2020-08-13 "},"Selenium/操作被定位的元素.html":{"url":"Selenium/操作被定位的元素.html","title":"操作被定位的元素","keywords":"","body":"操作被定位的元素 在讲述完定位对象(locate elements)之后我们需要对该已定位对象进行操作，通常所有的操作与页面交互都将通过WebElement接口，常见的操作元素方法如下： clear 清除元素的内容 send_keys 模拟按键输入 click 点击元素 submit 提交表单 submit表单 from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.action_chains import ActionChains import time driver = webdriver.Chrome() driver.get(\"http://www.python.org\") elem = driver.find_element_by_name(\"q\") elem.clear() elem.send_keys(\"selenium\") elem.submit() time.sleep(2) driver.back() WebElement接口获取值 通过WebElement接口可以获取常用的值，这些值同样非常重要。 size 获取元素的尺寸 text 获取元素的文本 get_attribute(name) 获取属性值 page_source 返回页面源码 tag_name返回元素的 tagName location 获取元素坐标，先找到要获取的元素，再调用该方法 driver.title 返回页面标题 current_url获取当前页面的URL is_displayed() 设置该元素是否可见 is_enabled() 判断元素是否被使用 is_selected() 判断元素是否被选中 测试： from selenium import webdriver from selenium.webdriver.common.keys import Keys import time driver = webdriver.PhantomJS(executable_path=\"G:\\phantomjs-1.9.1-windows\\phantomjs.exe\") driver.get(\"http://www.baidu.com/\") size = driver.find_element_by_name(\"wd\").size print size #尺寸: {'width': 500, 'height': 22} news = driver.find_element_by_xpath(\"//div[@id='u1']/a[1]\").text print news #文本: 新闻 href = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('href') name = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('name') print href,name #属性值: http://www.hao123.com/ tj_trhao123 location = driver.find_element_by_xpath(\"//div[@id='u1']/a[3]\").location print location #坐标: {'y': 19, 'x': 498} print driver.current_url #当前链接: https://www.baidu.com/ print driver.title #标题: 百度一下， 你就知道 result = location = driver.find_element_by_id(\"su\").is_displayed() print result #是否可见: True 其中图片解释如下图所示: 鼠标操作 在现实的自动化测试中关于鼠标的操作不仅仅是click()单击操作，还有很多包含在ActionChains类中的操作。如下： context_click(elem) 右击鼠标点击元素elem，另存为等行为 double_click(elem) 双击鼠标点击元素elem，地图web可实现放大功能 drag_and_drop(source,target) 拖动鼠标，源元素按下左键移动至目标元素释放 move_to_element(elem) 鼠标移动到一个元素上 click_and_hold(elem) 按下鼠标左键在一个元素上 perform() 在通过调用该函数执行ActionChains中存储行为 键盘操作 参考：http://selenium-python.readthedocs.org/api.html 前面讲述了鼠标操作，现在讲述键盘操作。在webdriver的Keys类中提供了键盘所有的按键操作，当然也包括一些常见的组合键操作如Ctrl+A(全选)、Ctrl+C(复制)、Ctrl+V(粘贴)。更多键参考官方文档对应的编码。 send_keys(Keys.ENTER) 按下回车键 send_keys(Keys.TAB) 按下Tab制表键 send_keys(Keys.SPACE) 按下空格键space send_keys(Kyes.ESCAPE) 按下回退键Esc send_keys(Keys.BACK_SPACE) 按下删除键BackSpace send_keys(Keys.SHIFT) 按下shift键 send_keys(Keys.CONTROL) 按下Ctrl键 send_keys(Keys.ARROW_DOWN) 按下鼠标光标向下按键 send_keys(Keys.CONTROL,'a') 组合键全选Ctrl+A send_keys(Keys.CONTROL,'c') 组合键复制Ctrl+C send_keys(Keys.CONTROL,'x') 组合键剪切Ctrl+X send_keys(Keys.CONTROL,'v') 组合键粘贴Ctrl+V 测试： from selenium import webdriver from selenium.webdriver.common.keys import Keys driver = webdriver.Chrome() driver.get(\"http://www.python.org\") assert \"Python\" in driver.title elem = driver.find_element_by_name(\"q\") elem.clear() elem.send_keys(\"pycon\") elem.send_keys(Keys.RETURN) assert \"No results found.\" not in driver.page_source driver.close() 具体参考 [python爬虫] Selenium常见元素定位方法和操作的学习介绍 Update time： 2020-08-13 "},"Selenium/selenium控制浏览器滚动条.html":{"url":"Selenium/selenium控制浏览器滚动条.html","title":"selenium控制浏览器滚动条","keywords":"","body":"selenium控制浏览器滚动条 指定下拉距离 需求：部分网站的数据是随着滚动条向下挥动加载的，爬取数据的需要将本页数据全部加载才可以进行翻页爬取。 目的：通过selenium控制浏览器滚动条 原理：通过 driver.execute_script()执行 js 代码，达到目的 driver.execute_script(\"window.scrollBy(0,1000)\") 语法：scrollBy(x,y) 参数 x 必需。向右滚动的像素值。 y 必需。向下滚动的像素值。 或者使用 js=\"var q=document.documentElement.scrollTop=10000\" driver.execute_script(js) 示例： from selenium import webdriver from lxml import etree from time import sleep url = 'https://search.jd.com/Search?keyword=mac&enc=utf-8&wq=mac&pvid=9862d03c24e741c6a58079d004f5aabf' chrome = webdriver.Chrome() chrome.get(url) js = 'document.documentElement.scrollTop=100000' chrome.execute_script(js) sleep(3) html = chrome.page_source e = etree.HTML(html) prices = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-price\"]/strong/i/text()') names = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-name p-name-type-2\"]/a/em') print(len(names)) for name, price in zip(names, prices): print(name.xpath('string(.)'), \":\", price) chrome.quit() 发送tab键，移动到目标元素 根据页面显示进行变通，发送tab键 可以通过tab键会切换到密码框中，所以根据此思路，在python中也可以发送tab键来切换，使元素显示. 用TAB键（或者向下键） 这里我们测试一下向下键（找到最底下的元素，离提交最近的，再来一次TAB），最后再点击： from selenium.webdriver.common.keys import Keys driver.find_element_by_id(\"id_login_method_0\").send_keys(Keys.TAB) 可以发送tab键来切换页面按钮，达到下拉滚动条的目的。但是一定要注意的是，指定的元素一定要能被TAB键选中，像输入框，超链接，Button等。 ```python from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.action_chains import ActionChains from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.ui import Select import time import random 加载xpath插件 chrome_options = webdriver.ChromeOptions() extension_path = 'D:/extension/XPath-Helper_v2.0.2.crx' chrome_options.add_extension(extension_path) browser = webdriver.Chrome(chrome_options=chrome_options) browser.maximize_window() wait = WebDriverWait(browser, 25) waitPopWindow = WebDriverWait(browser, 25) browser.get(\"https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=phone\") time.sleep(random.randrange(5, 10, 1)) 找到 Next Page 按钮， 属于可见元素 指定元素是 超链接 ———— 可以用Tab键切换到 targetElem = browser.find_element_by_xpath(\"//a[@id='pagnNextLink']\") 这个元素不是超链接，所以无法接收Tab键 targetElem = browser.find_element_by_xpath(\"//a[@id='pagnNextLink']/span[@id='pagnNextString']\") targetElem.send_keys(Keys.TAB) print(f\"结束拖动滚动条....\") time.sleep(random.randrange(5, 10, 1)) browser.quit() ### 拖动到指定元素位置。 ```python from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.action_chains import ActionChains from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.ui import Select import time import random #加载xpath插件 chrome_options = webdriver.ChromeOptions() extension_path = 'D:/extension/XPath-Helper_v2.0.2.crx' chrome_options.add_extension(extension_path) browser = webdriver.Chrome(chrome_options=chrome_options) #browser.maximize_window() wait = WebDriverWait(browser, 25) waitPopWindow = WebDriverWait(browser, 25) browser.get(\"https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=phone\") time.sleep(random.randrange(5, 10, 1)) #找到 Next Page 按钮， 属于可见元素 #js代码有两种写法，但是对元素的要求不同，focus更为严格 #第一种方法：focus targetElem = browser.find_element_by_xpath(\"//a[@id='pagnNextLink']\") browser.execute_script(\"arguments[0].focus();\", targetElem) #第二种方法：scrollIntoView #targetElem = browser.find_element_by_xpath(\"//a[@id='pagnNextLink']/span[@id='pagnNextString']\") #browser.execute_script(\"arguments[0].scrollIntoView();\", targetElem) # 拖动到可见的元素去 print(f\"结束拖动滚动条....\") time.sleep(random.randrange(5, 10, 1)) browser.quit() 参考 selenium_通过selenium控制浏览器滚动条 selenium 如何控制滚动条逐步滚动 python + selenium + chrome 如何操作滚动条 Update time： 2020-08-15 "},"Selenium/等待页面加载完成(Waits.html":{"url":"Selenium/等待页面加载完成(Waits.html","title":"等待页面加载完成(Waits)","keywords":"","body":"等待页面加载完成(Waits) selenium 的显示等待与隐式等待 现在很多的网页都采用了 Ajax 技术，那么采用一般的静态爬虫技术会出现抓取不到页面的元素。比如歌曲的主页会有评论数量，一般评论数量是动态加载的。 所以这就涉及到selenium,支持各种浏览器，包括Chrome，Safari，Firefox 等主流界面式浏览器，如果你在这些浏览器里面安装一个 Selenium 的插件，那么便可以方便地实现Web界面的测试。 driver = webdriver.Chrome() driver.get(\"http://somedomain/url_that_delays_loading\") driver.page_source #获取网页渲染后的源代码 动态加载的页面需要时间等待页面上的所有元素都渲染完成，如果在没有渲染完成之前我们就switch_to_或者是find_elements_by_，那么就可能出现元素定位困难而且会提高产生 ElementNotVisibleException 的概率。 直接找到我们要抓取的tag或者直接没有等待元素出来就开始交互导致不起作用的问题。 隐式等待 如果某些元素不是立即可用的，隐式等待是告诉WebDriver去等待一定的时间后去查找元素。 默认等待时间是0秒，一旦设置该值，隐式等待是设置该WebDriver的实例的生命周期。 from selenium import webdriver driver = webdriver.Firefox() driver.implicitly_wait(10) # seconds driver.get(\"http://somedomain/url_that_delays_loading\") myDynamicElement = driver.find_element_by_id(\"myDynamicElement\") 显示等待 显式等待是你在代码中定义等待一定条件发生后再进一步执行你的代码。 最糟糕的案例是使用time.sleep()，它将条件设置为等待一个确切的时间段。 这里有一些方便的方法让你只等待需要的时间。WebDriverWait结合ExpectedCondition 是实现的一种方式。 指定某个条件，然后设置最长等待时间。如果在这个时间还没有找到元素，那么便会抛出异常。只有该条件触发，才执行后续代码，这个使用更灵活。 主要涉及到selenium.webdriver.support 下的expected_conditions类。 example driver = webdriver.Chrome() driver.get(\"http://somedomain/url_that_delays_loading\") try: element = WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.ID, \"myDynamicElement\")) ) finally: driver.quit() example2 wait_result = WebDriverWait(driver=self.driver, timeout=300, poll_frequency=0.5, ignored_exceptions=None) .until( EC.text_to_be_present_in_element((By.XPATH, '//*[@id=\"VolumeTable\"]/tbody/tr[1]/td[4]/label'), u'可用')) 这里的 presence_of_element_located(())、text_to_be_present_in_element(())是其中两种方式，其实还有其他相关方式。EC 配合使用的 until() 或者 until_not() 方法说明： until(method, message='') 调用该方法体提供的回调函数作为一个参数，直到返回值为True until_not(method, message='') 调用该方法体提供的回调函数作为一个参数，直到返回值为False 模块包含一套预定义的条件集合。大大方便了 WebDriverWait 的使用。 Expected Conditions 类提供的预期条件判断方法 在进行浏览器自动化的时候，有一些条件是经常出现的，下面列出的是每个条件的实现。Selenium Python binding provides some convienence 提供了很多实用的方法。 title_is：判断当前页面的title是否等于预期 title_contains：判断当前页面的title是否包含预期字符串 presence_of_element_located：判断某个元素是否被加到了dom树里，并不代表该元素一定可见 visibility_of_element_located：判断某个元素是否可见. 可见代表元素非隐藏，并且元素的宽和高都不等于0 visibility_of：跟上面的方法做一样的事情，只是上面的方法要传入locator，这个方法直接传定位到的element就好了 presence_of_all_elements_located：判断是否至少有1个元素存在于dom树中。举个例子，如果页面上有n个元素的class都是'column-md-3'，那么只要有1个元素存在，这个方法就返回True text_to_be_present_in_element：判断某个元素中的text是否 包含 了预期的字符串 text_to_be_present_in_element_value：判断某个元素中的value属性是否包含了预期的字符串 frame_to_be_available_and_switch_to_it：判断该frame是否可以switch进去，如果可以的话，返回True并且switch进去，否则返回False invisibility_of_element_located：判断某个元素中是否不存在于dom树或不可见 element_to_be_clickable - it is Displayed and Enabled：判断某个元素中是否可见并且是enable的，这样的话才叫clickable staleness_of：等某个元素从dom树中移除，注意，这个方法也是返回True或False element_to_be_selected：判断某个元素是否被选中了,一般用在下拉列表 element_located_to_be_selected element_selection_state_to_be：判断某个元素的选中状态是否符合预期 element_located_selection_state_to_be：跟上面的方法作用一样，只是上面的方法传入定位到的element，而这个方法传入locator alert_is_present：判断页面上是否存在alert 参数1：By类确定哪种选择方式 from selenium.webdriver.common.by import By 参数2：值，可能是 xpath的值，可能是id,name等，取决于前面是By.XPATH, By.ID究竟是哪种方式去定位元素。 可以在 WebDriverWait()构造时传入下面参数，哪一个浏览器，来控制超时时间，多长时间检测一次这个元素是否加载，是否有异常报出。 driver：浏览器驱动 timeout：最长超时等待时间 poll_frequency：检测的时间间隔，默认为500ms ignore_exception：超时后抛出的异常信息，默认情况下抛 NoSuchElementException 异常 基本使用： from selenium import webdriver from selenium.webdriver.support.ui import WebDriverWait # 显示等待 from selenium.webdriver.support import expected_conditions as EC # 设置等待执行语句 from selenium.webdriver.common.by import By from selenium.common.exceptions import TimeoutException from time import sleep url = 'http://tieba.baidu.com/p/5923312469' options = webdriver.ChromeOptions() options.add_extension(r'D:\\python_demo\\AdBlock_v3.10.0.crx') # 添加插件 driver = webdriver.Chrome(chrome_options=options) driver.set_page_load_timeout(10) try: driver.get(url) WebDriverWait(driver, 10).until( # 必须是元组 EC.presence_of_element_located((By.CLASS_NAME, 'core_title_txt ')) ) print(driver.page_source) except TimeoutException as e: print('错误1') print(e) print(driver.page_source) 案例 爬取地理坐标 from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import re, pandas as pd def coordinate(site): # 创建浏览器驱动对象 driver = webdriver.Firefox() driver.get('http://api.map.baidu.com/lbsapi/getpoint/index.html') # 显式等待，设置timeout wait = WebDriverWait(driver, 9) # 判断输入框是否加载 input = wait.until( EC.presence_of_element_located( (By.CSS_SELECTOR, '#localvalue'))) # 判断搜索按钮是否加载 submit = wait.until( EC.element_to_be_clickable( (By.CSS_SELECTOR, '#localsearch'))) # 输入搜索词，点击搜索按钮 input.clear() input.send_keys(site) submit.click() # 等待坐标 wait.until( EC.presence_of_element_located( (By.CSS_SELECTOR, '#no_0'))) # 获取网页文本，提取经纬度 source = driver.page_source xy = re.findall('坐标：([\\d.]+),([\\d.]+)', source) # 转浮点数，取中位数 df = pd.DataFrame(xy, columns=['longitude', 'latitude']) df['longitude'] = pd.to_numeric(df['longitude']) df['latitude'] = pd.to_numeric(df['latitude']) longitude = df['longitude'].median() latitude = df['latitude'].median() # 关闭浏览器驱动 driver.close() return [longitude, latitude] print(coordinate('南海桂城地铁站')) # [113.1611575, 23.044811000000003] 参考 selenium 的显示等待与隐式等待 Update time： 2020-08-13 "},"Selenium/切换窗口.html":{"url":"Selenium/切换窗口.html","title":"切换窗口","keywords":"","body":"切换窗口 方法一 使用场景： 打开多个窗口，需要定位到新打开的窗口 使用方法： # 获取打开的多个窗口句柄 windows = driver.window_handles # 切换到当前最新打开的窗口 driver.switch_to.window(windows[-1]) 举例说明： from selenium import webdriver import time # 打开课工场网站主页【第一个窗口】 driver = webdriver.Chrome() driver.get('http://www.kgc.cn/') driver.maximize_window() # 点击全部课程，进入课程库【第二个窗口】 driver.find_element_by_link_text('[媒体] 15名学员入职阿里，课工场以实力成就高薪就业').click() time.sleep(3) # 使用第一种方法切换浏览器【切换到第二个窗口】 windows = driver.window_handles driver.switch_to.window(windows[-1]) time.sleep(3) # 点击课程库中的某个课程，进入课程详情界面【在第二个窗口页面进行元素点击操作，来判断窗口是否切换成功】 driver.find_element_by_xpath('//*[@id=\"yw1\"]/ul/li[2]').click() time.sleep(3) # 关闭浏览器 driver.quit() print('测试通过') 窗口归类 # 你打开的浏览器，谷歌 browser = webdriver.Chrome() # 你中间的操作 ... # 获取当前浏览器所有的窗口 handles = browser.window_handles # handles为一个数组：handles = [窗口1，窗口2，...] # 窗口切换，切换为新打开的窗口 browser.switch_to_window(handles[-1]) # 切换回最初打开的窗口 browser.switch_to_window(handles[0]) # 新增一个窗口打开url newwindow='window.open(\"https://www.baidu.com\");' browser.execute_script(newwindow) # 关闭当前窗口 browser.close() # 关闭所有窗口 browser.quit() 方法二 使用场景： 打开两个窗口，需要定位到新打开的窗口 使用方法： # 获得打开的第一个窗口句柄 window_1 = driver.current_window_handle # 获得打开的所有的窗口句柄 windows = driver.window_handles # 切换到最新的窗口 for current_window in windows: if current_window != window_1: driver.switch_to.window(current_window) 举例说明： from selenium import webdriver import time # 打开课工场网站主页【第一个窗口】 driver = webdriver.Chrome() driver.get('http://www.kgc.cn/') driver.maximize_window() # 点击全部课程，进入课程库【第二个窗口】 driver.find_element_by_link_text('全部课程').click() time.sleep(3) # 使用第二种方法切换浏览器【切换到第二个窗口】 window_1 = driver.current_window_handle windows = driver.window_handles for current_window in windows: if current_window != window_1: driver.switch_to.window(current_window) time.sleep(3) # 点击课程库中的某个课程，进入课程详情界面【在第二个窗口页面进行元素点击操作，来判断窗口是否切换成功】 driver.find_element_by_xpath('//*[@id=\"yw1\"]/ul/li[2]').click() time.sleep(3) # 关闭浏览器 driver.quit() print('测试通过') 两种方法的区别 1、第一种方法比较简单，能提升整体代码的性能 2、第二种方法是大家最常用的方法，比较容易理解 参考 python3 selenium 切换窗口的几种方法 selenium_多窗口切换整理--第二节 Update time： 2020-08-13 "},"Selenium/Chrome无头模式与操作窗口.html":{"url":"Selenium/Chrome无头模式与操作窗口.html","title":"Chrome无头模式与操作窗口","keywords":"","body":"Chrome无头模式与操作窗口 Chrome无头模式–headless 在做爬虫时，通常是不需要打开浏览器的，只需要使用浏览器的内核，因此可以使用Chrome的无头模式 from selenium import webdriver from selenium.webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_argument('--headless') driver = webdriver.Chrome(chrome_options=chrome_options) driver.get(\"http://www.baidu.com\") driver.close() 操作浏览器窗口 from selenium import webdriver import time driver = webdriver.Chrome() driver.get(\"http://www.baidu.com\") 全屏 driver.fullscreen_window() 最大化窗口 driver.maximize_window() 最小化窗口 driver.minimize_window() 获取窗口位置 获取窗口大小 同时获取窗口位置和大小 print(driver.get_window_position()) print(driver.get_window_size()) print(driver.get_window_rect()) 设置窗口位置 设置窗口大小 同时设置窗口位置和大小 driver.set_window_position(30,30) driver.set_window_size(500,500) driver.set_window_rect(10,10,1050,708) 参考： Python爬虫之selenium库(三)：Chrome无头模式与操作浏览器 Update time： 2020-08-13 "},"Selenium/selenium时间日期控件处理.html":{"url":"Selenium/selenium时间日期控件处理.html","title":"selenium时间日期控件处理","keywords":"","body":"selenium时间日期控件处理 背景介绍 我们在使用selenium爬取数据时，有时会需要选择日期，来获取某个时间段的数据。但是网上的日期控件还真是五花八门，有正常一点的： 淘宝联盟上的有这样的： 当然还有这样的： 简单点的，我们还可以模拟鼠标点击，拖动的方式。但是复杂点的，那就完蛋了，该怎么办呢？ 其实很简单，不管我们是通过什么方式选的，最终往服务器上发送的都是我们选定的日期数据，那么我们就不用去搞什么时间日期控件了，好好研究一下，我们手动选择的日期数据，是存储在页面的哪个位置，又是如何发送给服务器的，那就简单了。 可喜的是，大部分的日期控件，我们都可以把它当成一个普通的input框处理，对value进行赋值操作。 也有一些类型的input框都是禁止手动输入的，那就用 js 代码把禁止输入的readonly属性去掉就好。 可以直接输入值，没有 readonly 属性的，直接输入值就可以了 有 readonly 属性的，先用 js 去掉 readonly 属性，然后直接输入日期文本内容： # 介绍4中操作方法 # js = \"document.getElementById('txtBeginDate').removeAttribute('readonly')\" # 1.原生js，移除属性 # js = \"$('input[id=txtBeginDate]').removeAttr('readonly')\" # 2.jQuery，移除属性 # js = \"$('input[id=txtBeginDate]').attr('readonly',false)\" # 3.jQuery，设置为false js = \"$('input[id=txtBeginDate]').attr('readonly','')\" # 4.jQuery，设置为空（同3） 使用js方法输入日期 from selenium import webdriver import time driver = webdriver.Chrome() url = \"https://www.12306.cn/index/\" driver.get(url) time.sleep(5) # 处理时间 # js 去掉 readonly 属性 js = 'document.getElementById(\"train_date\").removeAttribute(\"readonly\");' driver.execute_script(js) # js 添加时间 js_value = 'document.getElementById(\"train_date\").value=\"2017-12-10\"' driver.execute_script(js_value) 参考 python下selenium如何处理日期控件的几种方法 python selenium 时间日期控件处理 Update time： 2020-08-15 "},"Selenium/selenium表格提交.html":{"url":"Selenium/selenium表格提交.html","title":"selenium表格提交","keywords":"","body":"selenium表格提交 1.第一步都是 import from selenium import webdriver import time driver = webdriver.Chrome() 2.登录，如果不需要登录则可以跳过这一步：先通过右键查看源码（关键段） 管理帐号 密码 selenium的工作原理就是首先定位一个元素，然后再对其进行操作。Lucky，这段源码有id值，我们知道在html里id都是唯一的。因此这里我们找到的两个ID：id=\"loginform-employee_no\" id=\"loginform-password\"可以用 elem_user = driver.find_element_by_id这一方法来定位。 然后是操作部分：用keys类来模拟键盘输入send（也可以是回车或者空格等等操作），先import Keys类： from selenium.webdriver.common.keys import Keys 综合一下： elem_user = driver.find_element_by_id(\"loginform-employee_no\") elem_user.send_keys(\"用户名\") elem_pwd = driver.find_element_by_id(\"loginform-password\") elem_pwd.send_keys(\"密码\") elem_pwd.send_keys(Keys.RETURN) #return和enter都是回车键，只是表达不同。 time.sleep(2) #sleep这一步跟网络环境有关，有时候如果输入太快，可能会引起报错。单位为秒。后面不再重复写。 3.登录后就可以开始我们的填表了，直接开车（drive）到填表页面（仅针对URL不变的填表地址，如果url是变动的需要另外处理这步driver.get）： driver.get(\"http://表单网址\") 我们来看下这个需要填的内容： 等级： 这个内容是个下拉菜单的类型： 兼职 见习 初级 中级 高级 好消息是这个下拉选择框有id，那么我么就可以像刚才那样的定位，但是坏消息是，这个下拉选择框的选项没有id，这时我们可以另一个定位方法，用tag name, find_elements_by_tag_name , 这里是 ”option“，并且是以类似 list 的方式存在的，所以引用标签是0-4 代表着五个选项。所以这里我们如果要选选项4的代码如下： driver.find_element_by_id(\"employee-worker_level\") .find_elements_by_tag_name(\"option\")[3].click() 这里的click代表的是左键单击。 工作角色：这个是个checkbox类的提交内容： 工作角色 宣传人员 收费人员 维护人员 安装人员 移机人员 综合人员 投诉专岗 新开销售 选择人员的工作角色，如果非实际工作人员，请留空 这里也是类似的，我们看到有label这个tag name所以可以直接用label name，但是，如果我们需要复选的话，很简单，用个循环就行了： i=0 while (i 其他的需要填的内容就和登录里需要的是类似的。 最后我们只需要用keys类回车一下，就可以提交表格了。 bonus： 好奇提交表格的时候能不能用click动作来完成，走向了作死之路： 我们先来看下提交的代码： 添加 等等，id呢？name呢？让我们看下其他的定位方法： 通过id定位元素：find_element_by_id(\"id_vaule\") 通过name定位元素：find_element_by_name(\"name_vaule\") 通过tag_name定位元素：find_element_by_tag_name(\"tag_name_vaule\") 通过class_name定位元素：find_element_by_class_name(\"class_name\") 通过css定位元素：find_element_by_css_selector(); 通过xpath定位元素：find_element_by_xpath(\"xpath\") 通过link定位：find_element_by_link_text(\"text_vaule\")或者find_element_by_partial_link_text() 好的，貌似目前只能用Xpath的方法来定位了 这里有一个教程：http://www.w3school.com.cn/xpath/index.asp Update time： 2020-08-15 "},"Selenium/selenium处理select标签下拉框的选项.html":{"url":"Selenium/selenium处理select标签下拉框的选项.html","title":"selenium处理select标签下拉框的选项","keywords":"","body":"selenium处理select标签下拉框的选项 1. 背景 在爬取网页是，有时候我们会遇到下图中的下拉框，也就是 标签。按照一般的点击方案是无法成功的，而selenium提供了专门的Select类来处理这种下拉框。 2. select下拉框的处理方案 上图中的网页源代码如下： All Departments Alexa Skills Amazon Devices Amazon Video Amazon Warehouse Deals Appliances Apps &amp; Games Arts, Crafts &amp; Sewing Automotive Parts &amp; Accessories Baby Beauty &amp; Personal Care Books CDs &amp; Vinyl Cell Phones &amp; Accessories Clothing, Shoes &amp; Jewelry &nbsp;&nbsp;&nbsp;Women &nbsp;&nbsp;&nbsp;Men &nbsp;&nbsp;&nbsp;Girls &nbsp;&nbsp;&nbsp;Boys &nbsp;&nbsp;&nbsp;Baby Collectibles &amp; Fine Art Computers Courses Credit and Payment Cards Digital Music Electronics Garden &amp; Outdoor Gift Cards Grocery &amp; Gourmet Food Handmade Health, Household &amp; Baby Care Home &amp; Business Services Home &amp; Kitchen Industrial &amp; Scientific Kindle Store Luggage &amp; Travel Gear Luxury Beauty Magazine Subscriptions Movies &amp; TV Musical Instruments Office Products Pet Supplies Prime Exclusive Savings Prime Pantry Software Sports &amp; Outdoors Tools &amp; Home Improvement Toys &amp; Games Vehicles Video Games 第一种方案：select_by_value 使用条件：用于选取标签的value值，要求必须要有value属性，当然，这不是废话嘛…。 ```python from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.action_chains import ActionChains from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.ui import Select import random import time browser = webdriver.Chrome() browser.get(\"https://www.amazom.com/\") 选中最后一个选项 Video Games classSelectValue = 'search-alias=videogames' Select(browser.find_element_by_tag_name(\"select\")).select_by_value(classSelectValue) 拿到搜索框 input = browser.find_element_by_xpath(\"//form[@name='site-search']/div[@class='nav-fill']//input[@class='nav-input']\") 输入搜索关键字 time.sleep(random.randrange(1, 5, 1)) input.clear() input.send_keys('mount') 敲enter键 input.send_keys(Keys.RETURN) ### 第二种方案：select_by_index 使用条件：要求下拉框的选项必须要有**index属性**，例如index=”1”。注意，这不是数组下标值，而是属性值！ ```python # 稍微变一下，这种方法在这不合适，仅仅是mark一下 Select(browser.find_element_by_tag_name(\"select\")).select_by_index(2) 第三种方案：select_by_visible_text 使用条件：用于选取标签的 text 值！ # Toys &amp; Games # 一定要注意是源代码中的text值，而不要从页面去复制，区别就在于，源代码中会有&amp classSelectText = 'Toys &amp; Games' Select(browser.find_element_by_tag_name(\"select\")).select_by_visible_text(classSelectText ) 参考 selenium + python处理select标签下拉框的选项 Update time： 2020-08-15 "},"Selenium/单选框和复选框radiobox、checkbox.html":{"url":"Selenium/单选框和复选框radiobox、checkbox.html","title":"单选框和复选框(radiobox、checkbox)","keywords":"","body":"单选框和复选框(radiobox、checkbox) 常规的单(复)选框 一、认识单选框和复选框 1.先认清楚单选框和复选框长什么样 上面的单选框是圆的；下图复选框是方的 二、radio和checkbox源码 1.上图的html源码如下，把下面这段复杂下来，写到文本里，后缀改成.html就可以了。 单选和复选 单选：性别 男 女 微信公众号：从零开始学自动化测试 checkbox1 --> selenium checkbox2 --> python checkbox3 --> appium Male Female --> 三、单选：radio 1.首先是定位选择框的位置 2.定位id，点击图标就可以了，代码如下（获取url地址方法：把上面源码粘贴到文本保存为.html后缀后用浏览器打开，在浏览器url地址栏复制出地址就可以了） 3.先点击boy后，等十秒再点击girl，观察页面变化 四、复选框：checkbox 1.勾选单个框，比如勾选selenium这个，可以根据它的id=c1直接定位到点击就可以了 那么问题来了:如果想全部勾选上呢？ 五、全部勾选： 1.全部勾选，可以用到定位一组元素，从上面源码可以看出，复选框的type=checkbox,这里可以用xpath语法：.//*[@type='checkbox'] 2.这里注意，敲黑板做笔记了：find_elements是不能直接点击的，它是复数的，所以只能先获取到所有的checkbox对象，然后通过for循环去一个个点击操作 六、判断是否选中：is_selected() 1.有时候这个选项框，本身就是选中状态，如果我再点击一下，它就反选了，这可不是我期望的结果，那么可不可以当它是没选中的时候，我去点击下；当它已经是选中状态，我就不点击呢？那么问题来了：如何判断选项框是选中状态？ 2.判断元素是否选中这一步才是本文的核心内容，点击选项框对于大家来说没什么难度。获取元素是否为选中状态，打印结果如下图。 3.返回结果为bool类型，没点击时候返回False,点击后返回True，接下来就很容易判断了，既可以作为操作前的判断，也可以作为测试结果的判断 代码： # coding:utf-8 from selenium import webdriver driver = webdriver.Firefox() driver.get(\"file:///C:/Users/Gloria/Desktop/checkbox.html\") # 没点击操作前，判断选项框状态 s = driver.find_element_by_id(\"boy\").is_selected() print s driver.find_element_by_id(\"boy\").click() # 点击后，判断元素是否为选中状态 r = driver.find_element_by_id(\"boy\").is_selected() print r # 复选框单选 driver.find_element_by_id(\"c1\").click() # 复选框全选 checkboxs = driver.find_elements_by_xpath(\".//*[@type='checkbox']\") for i in checkboxs: i.click() 特殊的单(复)选框 例如 12306 的复选框 标签没有 type=\"checkbox\" 属性，而是通过改变 class 的值，来设置是否勾选： 解决方案： 用 driver.execute_script()这个方法来执行 js 语句, 改变属性的值，达到勾选的目的。 代码： from selenium import webdriver import time from selenium.webdriver.common.action_chains import ActionChains driver = webdriver.Chrome() url = \"https://www.12306.cn/index/\" driver.get(url) time.sleep(5) # 设置出发地 s = driver.find_element_by_id('fromStationText') ActionChains(driver).move_to_element(s) \\ .click(s) \\ .send_keys_to_element(s, \"南京\") \\ .move_by_offset(20, 50) \\ .click() \\ .perform() # 设置目的地 t = driver.find_element_by_id('toStationText') ActionChains(driver).move_to_element(t) \\ .click(t) \\ .send_keys_to_element(t, \"北京\") \\ .move_by_offset(20, 50) \\ .click() \\ .perform() # 处理时间 # js 去掉 readonly 属性 js = 'document.getElementById(\"train_date\").removeAttribute(\"readonly\");' driver.execute_script(js) # js 添加时间 js_value = 'document.getElementById(\"train_date\").value=\"2020-08-20\"' driver.execute_script(js_value) js = \"document.getElementById('isStudentDan').setAttribute('class', 'active')\" driver.execute_script(js) 参考 Selenium：是否可以在Selenium中设置WebElement的任何属性值？ Selenium2学习（十五）-- 单选框和复选框（radiobox、checkbox） Update time： 2020-08-14 "},"Selenium/selenium给元素的属性赋值.html":{"url":"Selenium/selenium给元素的属性赋值.html","title":"selenium给元素的属性赋值","keywords":"","body":"selenium给元素的属性赋值 原帖： 我有一个WebElement，我想将其属性值重置为其他值（例如attr是该属性，并且我想将其原始value=1更改为新的value=10）。 可能吗？我正在使用Selenium 2.0（WebDriver。） 必须使用JavascriptExecutor类： WebDriver driver; // Assigned elsewhere JavascriptExecutor js = (JavascriptExecutor) driver; js.executeScript(\"document.getElementById('//id of element').setAttribute('attr', '10')\"); 爬虫的时候遇到了这样的情况； 用 driver.execute_script()这个方法来执行 js 语句, 改变属性的值，达到勾选的目的。 js = \"document.getElementById('isStudentDan').setAttribute('class', 'active')\" driver.execute_script(js) 特殊的单(复)选框 例如 12306 的复选框 标签没有 type=\"checkbox\" 属性，而是通过改变 class 的值，来设置是否勾选： 解决方案： 用 driver.execute_script()这个方法来执行 js 语句, 改变属性的值，达到勾选的目的。 代码： from selenium import webdriver import time from selenium.webdriver.common.action_chains import ActionChains driver = webdriver.Chrome() url = \"https://www.12306.cn/index/\" driver.get(url) time.sleep(5) # 设置出发地 s = driver.find_element_by_id('fromStationText') ActionChains(driver).move_to_element(s) \\ .click(s) \\ .send_keys_to_element(s, \"南京\") \\ .move_by_offset(20, 50) \\ .click() \\ .perform() # 设置目的地 t = driver.find_element_by_id('toStationText') ActionChains(driver).move_to_element(t) \\ .click(t) \\ .send_keys_to_element(t, \"北京\") \\ .move_by_offset(20, 50) \\ .click() \\ .perform() # 处理时间 # js 去掉 readonly 属性 js = 'document.getElementById(\"train_date\").removeAttribute(\"readonly\");' driver.execute_script(js) # js 添加时间 js_value = 'document.getElementById(\"train_date\").value=\"2020-08-20\"' driver.execute_script(js_value) js = \"document.getElementById('isStudentDan').setAttribute('class', 'active')\" driver.execute_script(js) 参考 Selenium：是否可以在Selenium中设置WebElement的任何属性值？ Update time： 2020-08-15 "},"Selenium/ActionChains行为连.html":{"url":"Selenium/ActionChains行为连.html","title":"ActionChains行为连","keywords":"","body":"ActionChains行为连 Update time： 2020-08-14 "},"Scrapy/":{"url":"Scrapy/","title":"Scrapy","keywords":"","body":"Scrapy Update time： 2020-05-27 "},"Scrapy/Scrapy介绍.html":{"url":"Scrapy/Scrapy介绍.html","title":"Scrapy介绍","keywords":"","body":"Scrapy介绍 写一个爬虫，需要做很多的事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换ip代理、设置请求头等）、异步请求等。这些工作如果每次都要自己从零开始写的话，比较浪费时间。因此Scrapy 把一些基础的东西封装好了，在他上面写爬虫可以变的更加的高效（爬取效率和开发效率）。因此真正在公司里，一些上了量的爬虫，都是使用Scrapy框架来解决。 Scrapy框架模块功能 Scrapy Engine（引擎）： Scrapy框架的核心部分。负责在Spider和 ItemPipeline、Downloader、Scheduler中间通信、传递数据等。 Spider（爬虫）：发送需要爬取的链接给引擎，最后引擎把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是我们需要的，都是由程序员自己决定。 Scheduler（调度器）：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。 Downloader（下载器）：负责接收引擎传过来的下载请求，然后去网络上下载对应的数据再交还给引擎。 Item Pipeline（管道）：负责将Spider（爬虫）传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求。 Downloader Middlewares（下载中间件）：可以扩展下载器和引擎之间通信功能的中间件。 Spider Middlewares（Spider中间件）：可以扩展引擎和爬虫之间通信功能的中间件。 安装和文档 安装 pip install scrapy \"\"\" 注意：如果在windows系统下，提示这个错误 ModuleNotFoundError: No module named ‘win32api’， 那么使用以下命令可以解决：pip install pypiwin32。 \"\"\" Scrapy官方文档 Scrapy中文文档 创建项目 要使用Scrapy框架创建项目，需要通过命令来创建。首先进入到你想把这个项目存放的目录。然后使用以下命令创建： #scrapy startproject [项目名称] scrapy startproject qss 目录结构介绍： 以下介绍下主要文件的作用： items.py：用来存放爬虫爬取下来数据的模型。 middlewares.py：用来存放各种中间件的文件。 pipelines.py：用来将items的模型存储到本地磁盘中。 settings.py：本爬虫的一些配置信息（比如请求头、多久发送一次请求、ip代理池等）。 scrapy.cfg：项目的配置文件。 spiders包：以后所有的爬虫，都是存放到这个里面。 定义Item Item 是保存爬取到的数据的容器；其使用方法和python字典类似， 并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。 可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field 的类属性来定义一个Item。 import scrapy class DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() 编写第一个爬虫(Spider) Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类 其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方法。 为了创建一个Spider，您必须继承 scrapy.Spider 类， 且定义以下三个属性: name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。 start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。 parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。 使用命令创建一个爬虫 首先进入项目的文件夹 cd spider scrapy genspider qsbk \"qiushibaike.com\" 创建了一个名字叫做 qsbk 的爬虫，并且能爬取的网页只会限制在 qiushibaike.com 这个域名下。 创建项目和爬虫： 创建项目：scrapy startproject [爬虫的名字]。 创建爬虫：进入到项目所在的路径，执行命令：scrapy genspider [爬虫名字] [爬虫的域名]。注意，爬虫名字不能和项目名称一致。 运行scrapy项目 进入项目的根目录，执行下列命令启动spider: # scrapy crawl [爬虫名字] scrapy crawl qsbk 如果不想每次都在命令行中运行，那么可以把这个命令写在一个文件中。以后就在pycharm中执行运行这个文件就可以了。比如现在新创建一个文件叫做start.py，然后在这个文件中填入以下代码： from scrapy import cmdline cmdline.execute(\"scrapy crawl qsbk\".split()) Update time： 2020-05-27 "},"Scrapy/Items.html":{"url":"Scrapy/Items.html","title":"Items","keywords":"","body":"Items 爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。 Scrapy提供 Item 类来满足这样的需求。 Item 对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like) 的API以及用于声明可用字段的简单语法。 声明Item Item使用简单的class定义语法以及 Field 对象来声明。例如: import scrapy class Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) 创建item >>> product = Product(name='Desktop PC', price=1000) >>> print product Product(name='Desktop PC', price=1000) 获取字段的值 >>> product['name'] Desktop PC >>> product.get('name') Desktop PC >>> product['price'] 1000 设置字段的值 >>> product['last_updated'] = 'today' >>> product['last_updated'] today 获取所有获取到的值 >>> product.keys() ['price', 'name'] >>> product.items() [('price', 1000), ('name', 'Desktop PC')] Update time： 2020-05-27 "},"Scrapy/Spiders.html":{"url":"Scrapy/Spiders.html","title":"Spiders","keywords":"","body":"Spiders Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。 对spider来说，爬取的循环类似下文: 以初始的URL初始化Request，并设置回调函数。 当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。 spider中初始的request是通过调用 start_requests() 来获取的。 start_requests() 读取 start_urls 中的URL， 并以 parse 为回调函数生成 Request 。 在回调函数内分析返回的(网页)内容，返回 Item 对象或者 Request 或者一个包括二者的可迭代容器。 返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。 在回调函数内，您可以使用 选择器(Selectors) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。 最后，由spider返回的item将被存到数据库(由某些 Item Pipeline 处理)或使用 Feed exports 存入到文件中。 Spider classscrapy.spider.Spider Spider是最简单的spider。每个其他的spider必须继承自该类(包括Scrapy自带的其他spider以及您自己编写的spider)。 Spider并没有提供什么特殊的功能。 其仅仅请求给定的 start_urls/start_requests ，并根据返回的结果(resulting responses)调用spider的 parse 方法。 name 定义spider名字的字符串(string)。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。 不过您可以生成多个相同的spider实例(instance)，这没有任何限制。 name是spider最重要的属性，而且是必须的。 allowed_domains 可选。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。 start_urls URL列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。 parse(response) 当response没有指定回调函数时，该方法是 Scrapy 处理下载的response的默认方法。 parse 负责处理response并返回处理的数据以及(/或)跟进的URL。 Spider 对其他的Request的回调函数也有相同的要求。 该方法及其他的Request回调函数必须返回一个包含 Request 及(或) Item 的可迭代的对象。 参数: response (Response) – 用于分析的response ，相当于request 库的返回值 可以执行 xpath 语法 Spider样例 import scrapy class MySpider(scrapy.Spider): name = 'example.com' allowed_domains = ['example.com'] start_urls = [ 'http://www.example.com/1.html', 'http://www.example.com/2.html', 'http://www.example.com/3.html', ] def parse(self, response): self.log('A response from %s just arrived!' % response.url) 另一个在单个回调函数中返回多个Request以及Item的例子: import scrapy from myproject.items import MyItem class MySpider(scrapy.Spider): name = 'example.com' allowed_domains = ['example.com'] start_urls = [ 'http://www.example.com/1.html', 'http://www.example.com/2.html', 'http://www.example.com/3.html', ] def parse(self, response): sel = scrapy.Selector(response) for h3 in response.xpath('//h3').extract(): yield MyItem(title=h3) for url in response.xpath('//a/@href').extract(): yield scrapy.Request(url, callback=self.parse) CrawlSpider classscrapy.contrib.spiders.CrawlSpider 爬取一般网站常用的spider。其定义了一些规则(rule)来提供跟进link的方便的机制。 也许该spider并不是完全适合您的特定网站或项目，但其对很多情况都使用。 因此您可以以其为起点，根据需求修改部分方法。当然您也可以实现自己的spider。 除了从Spider继承过来的(您必须提供的)属性外，其提供了一个新的属性: rules 一个包含一个(或多个) Rule 对象的集合(list)。 每个 Rule 对爬取网站的动作定义了特定表现。 Rule对象在下边会介绍。 如果多个rule匹配了相同的链接，则根据他们在本属性中被定义的顺序，第一个会被使用。 parse_start_url(response) 当start_url的请求返回时，该方法被调用。 该方法分析最初的返回值并必须返回一个 Item 对象或者 一个 Request 对象或者 一个可迭代的包含二者对象。 爬取规则(Crawling rules) classscrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None) link_extractor 是一个 Link Extractor 对象。 其定义了如何从爬取到的页面提取链接。 callback 是一个callable或string(该spider中同名的函数将会被调用)。 从link_extractor中每获取到链接时将会调用该函数。该回调函数接受一个response作为其第一个参数， 并返回一个包含 Item 以及(或) Request 对象(或者这两者的子类)的列表(list)。 cb_kwargs 包含传递给回调函数的参数(keyword argument)的字典。 follow 是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果 callback 为None， follow 默认设置为 True ，否则默认为 False 。 process_links 是一个callable或string(该spider中同名的函数将会被调用)。 从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。 process_request 是一个callable或string(该spider中同名的函数将会被调用)。 该规则提取到每个request时都会调用该函数。该函数必须返回一个request或者None。 (用来过滤request) CrawlSpider样例 import scrapy from scrapy.contrib.spiders import CrawlSpider, Rule from scrapy.contrib.linkextractors import LinkExtractor class MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # 提取匹配 'category.php' (但不匹配 'subsection.php') 的链接并跟进链接(没有callback意味着follow默认为True) Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))), # 提取匹配 'item.php' 的链接并使用spider的parse_item方法进行分析 Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'), ) def parse_item(self, response): self.log('Hi, this is an item page! %s' % response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)') item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').extract() item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').extract() return item 该spider将从example.com的首页开始爬取，获取category以及item的链接并对后者使用 parse_item 方法。 当item获得返回(response)时，将使用XPath处理HTML并生成一些数据填入 Item 中。 Update time： 2020-05-27 "},"Scrapy/Item Pipeline.html":{"url":"Scrapy/Item Pipeline.html","title":"Item Pipeline","keywords":"","body":"Item Pipeline 当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。 每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。 以下是item pipeline的一些典型应用： 清理HTML数据 验证爬取的数据(检查item包含某些字段) 查重(并丢弃) 将爬取结果保存到数据库中 编写你自己的item pipeline 编写你自己的item pipeline很简单，每个item pipeline组件是一个独立的Python类，同时必须实现以下方法: process_item(self, item**, spider) 每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item** (或任何继承类)对象， 或是抛出 DropItem 异常，被丢弃的item将不会被之后的pipeline组件所处理。 参数: item (Item 对象) – 被爬取的item spider (Spider 对象) – 爬取该item的spider 此外,他们也可以实现以下方法: open_spider(self, spider) 当spider被开启时，这个方法被调用。 参数: spider (Spider 对象) – 被开启的spider close_spider(spider) 当spider被关闭时，这个方法被调用 参数: spider (Spider 对象) – 被关闭的spider Item pipeline 样例 验证价格，同时丢弃没有价格的item 让我们来看一下以下这个假设的pipeline，它为那些不含税(price_excludes_vat 属性)的item调整了 price 属性，同时丢弃了那些没有价格的item: from scrapy.exceptions import DropItem class PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item['price']: if item['price_excludes_vat']: item['price'] = item['price'] * self.vat_factor return item else: raise DropItem(\"Missing price in %s\" % item) 将item写入JSON文件 以下pipeline将所有(从所有spider中)爬取到的item，存储到一个独立地 items.jl 文件，每行包含一个序列化为JSON格式的item: import json class JsonWriterPipeline(object): def __init__(self): self.file = open('items.jl', 'wb') def process_item(self, item, spider): line = json.dumps(dict(item)) + \"\\n\" self.file.write(line) return item Write items to MongoDB import pymongo class MongoPipeline(object): def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'items') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): collection_name = item.__class__.__name__ self.db[collection_name].insert(dict(item)) return item 启用一个Item Pipeline组件 ITEM_PIPELINES = { 'myproject.pipelines.PricePipeline': 300, 'myproject.pipelines.JsonWriterPipeline': 800, } 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。 pipeline区分传来Items 各个页面都会封装items并将item传递给pipelines来处理，而pipelines接收的入口只有一个就是 def process_item(self, item, spider): pass spider对应相应的爬虫，调用spider.name也可区分来自不同爬虫的item def process_item(self, item, spider): if spider.name == \"XXXX\": pass Update time： 2020-05-27 "},"Scrapy/下载项目图片.html":{"url":"Scrapy/下载项目图片.html","title":"下载项目图片","keywords":"","body":"下载项目图片 Scrapy提供了一个 item pipeline ，来下载属于某个特定项目的图片，比如，当你抓取产品时，也想把它们的图片下载到本地。 这条管道，被称作图片管道，在 ImagesPipeline 类中实现，提供了一个方便并具有额外特性的方法，来下载并本地存储图片: 将所有下载的图片转换成通用的格式（JPG）和模式（RGB） 避免重新下载最近已经下载过的图片 缩略图生成 检测图像的宽/高，确保它们满足最小限制 这个管道也会为那些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到那个队列中。 这可以避免多次下载几个项目共享的同一个图片。 使用图片管道 当使用 ImagesPipeline ，典型的工作流程如下所示: 在一个爬虫里，你抓取一个项目，把其中图片的URL放入 image_urls 组内。 项目从爬虫内返回，进入项目管道。 当项目进入 ImagesPipeline，image_urls 组内的URLs将被Scrapy的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持“locker”的状态，直到完成图片的下载（或者由于某些原因未完成下载）。 当图片下载完，另一个组(images)将被更新到结构中。这个组将包含一个字典列表，其中包括下载图片的信息，比如下载路径、源抓取地址（从 image_urls 组获得）和图片的校验码。 images 列表中的图片顺序将和源 image_urls 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 images 组中。 使用样例 为了使用图片管道，你仅需要 启动它 并用 image_urls 和 images 定义一个项目: import scrapy class MyItem(scrapy.Item): # ... other item fields ... image_urls = scrapy.Field() images = scrapy.Field() 如果你需要更加复杂的功能，想重写定制图片管道行为，参见 实现定制图片管道 。 开启你的图片管道 为了开启你的图片管道，你首先需要在项目中添加它 ITEM_PIPELINES setting: ITEM_PIPELINES = {'scrapy.contrib.pipeline.images.ImagesPipeline': 1} 并将 IMAGES_STORE 设置为一个有效的文件夹，用来存储下载的图片。否则管道将保持禁用状态，即使你在 ITEM_PIPELINES 设置中添加了它。 比如: IMAGES_STORE = '/path/to/valid/dir' 图片存储 文件系统存储 图片存储在文件中（一个图片一个文件），并使用它们URL的 SHA1 hash 作为文件名。 比如，对下面的图片URL: http://www.example.com/image.jpg 它的 SHA1 hash 值为: 3afec3b4765f8f0a07b78f98c07b83f013567a0a 将被下载并存为下面的文件: /full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg 实现定制图片管道 下面是你可以在定制的图片管道里重写的方法： classscrapy.contrib.pipeline.images.ImagesPipeline get_media_requests(item, info**)** 在工作流程中可以看到，管道会得到图片的URL并从项目中下载。为了这么做，你需要重写 get_media_requests() 方法，并对各个图片URL返回一个Request: def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) 这些请求将被管道处理，当它们完成下载后，结果将以2-元素的元组列表形式传送到 item_completed() 方法: 默认 get_media_requests() 方法返回 None ，这意味着项目中没有图片可下载。 item_completed(results, items**, info)** 当一个单独项目中的所有图片请求完成时（要么完成下载，要么因为某种原因下载失败）， ImagesPipeline.item_completed() 方法将被调用。 item_completed() 方法需要返回一个输出，其将被送到随后的项目管道阶段，因此你需要返回（或者丢弃）项目，如你在任意管道里所做的一样。 这里是一个 item_completed() 方法的例子，其中我们将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: from scrapy.exceptions import DropItem def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item 默认情况下， item_completed() 方法返回项目。 定制图片管道的例子 import scrapy from scrapy.contrib.pipeline.images import ImagesPipeline from scrapy.exceptions import DropItem class MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item Update time： 2020-05-27 "},"Scrapy/Scrapy的Request和Response.html":{"url":"Scrapy/Scrapy的Request和Response.html","title":"Scrapy的Request和Response","keywords":"","body":"Scrapy的Request和Response 请求和响应 Scrapy的Request 和Response对象用于爬网网站。 通常，Request对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个Response对象，该对象返回到发出请求的爬虫程序。 两个类Request和Response类都有一些子类，它们添加基类中不需要的功能。这些在下面的请求子类和 响应子类中描述。 class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback] ) 一个Request对象表示一个HTTP请求，它通常是在爬虫生成，并由下载执行，从而生成Response。 常用参数： url（string） - 此请求的网址 callback（callable） - 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的将附加数据传递给回调函数。如果请求没有指定回调，parse()将使用spider的 方法。请注意，如果在处理期间引发异常，则会调用errback。 method（string） - 此请求的HTTP方法。默认为’GET’。 meta（dict） - 属性的初始值Request.meta。如果给定，在此参数中传递的dict将被浅复制。 headers（dict） - 这个请求的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None作为值传递，则不会发送HTTP头。 body（str或unicode） - 请求体。如果unicode传递了a，那么它被编码为 str使用传递的编码（默认为utf-8）。如果 body没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个str（不会是unicode或None）。 cookie（dict或list） - 请求cookie。这些可以以两种形式发送。 dont_filter（boolean） - 表示此请求不应由调度程序过滤。当您想要多次执行相同的请求时忽略重复过滤器时使用。小心使用它，或者你会进入爬行循环。默认为False。 priority（int） - 此请求的优先级（默认为0）。调度器使用优先级来定义用于处理请求的顺序。具有较高优先级值的请求将较早执行。允许负值以指示相对低优先级。 encoding（string） - 此请求的编码（默认为’utf-8’）。此编码将用于对URL进行百分比编码，并将正文转换为str（如果给定unicode）。 Request中meta参数的作用是传递信息给下一个函数，使用过程可以理解成： 把需要传递的信息赋值给这个叫meta的变量， 但meta只接受字典类型的赋值，因此 要把待传递的信息改成“字典”的形式，即： meta={'key1':value1,'key2':value2} 如果想在下一个函数中取出value1, 只需得到上一个函数的meta['key1']即可， 因为meta是随着Request产生时传递的， 下一个函数得到的Response对象中就会有meta， 即response.meta， 取value1则是value1=response.meta['key1'] class example(scrapy.Spider): name='example' allowed_domains=['example.com'] start_urls=['http://www.example.com'] def parse(self,response): #从start_urls中分析出的一个网址赋值给url url=response.xpath('.......').extract() #ExamleClass是在items.py中定义的,下面会写出。 \"\"\"记住item本身是一个字典\"\"\" item=ExampleClass() item['name']=response.xpath('.......').extract() item['htmlurl']=response.xpath('.......').extract() \"\"\"通过meta参数，把item这个字典，赋值给meta中的'key'键（记住meta本身也是一个字典）。 Scrapy.Request请求url后生成一个\"Request对象\"，这个meta字典（含有键值'key'，'key'的值也是一个字典，即item） 会被“放”在\"Request对象\"里一起发送给parse2()函数 \"\"\" yield Request(url,meta={'key':item},callback='parse2') def parse2(self,response): item=response.meta['key'] \"\"\"这个response已含有上述meta字典，此句将这个字典赋值给item， 完成信息传递。这个item已经和parse中的item一样了\"\"\" item['text']=response.xpath('.......').extract() #item共三个键值，到这里全部添加完毕了 yield item meta是浅复制，必要时需要深复制。 import copy meta={'key':copy.deepcopy('value')} meta是一个dict，主要是用解析函数之间传递值，一种常见的情况：在parse中给item某些字段提取了值，但是另外一些值需要在parse_item中提取，这时候需要将parse中的item传到parse_item方法中处理，显然无法直接给parse_item设置而外参数。 Request对象接受一个meta参数，一个字典对象，同时Response对象有一个meta属性可以取到相应request传过来的meta。所以解决上述问题可以这样做： def parse(self, response): # item = ItemClass() yield Request(url, meta={'item': item}, callback=self.parse_item) def parse(self, response): item = response.meta['item'] item['field'] = value yield item Request和Response之间如何传参 有些时候需要将两个页面的内容合并到一个item里面，这时候就需要在yield scrapy.Request的同时，传递一些参数到一下页面中。这时候可以这样操作。 request=scrapy.Request(houseurl,method='GET',callback=self.showhousedetail) request.meta['biid']=biid yield request def showhousedetail(self,response): house=HouseItem() house['bulidingid']=response.meta['biid'] Response 对象 Response 对象一般是有 scrapy 自动构建的，因此不用关心如何构建，二十如何使用它，Response 有很多属性可以用来获取数据，主要有以下属性： meta ： 其他请求传过来的 meta 属性，可以保持多个请求之间的数据连接 encoding : 返回当前字符串编码和解码的格式 text 将返回的数据作为 Unicode 字符返回 body : 将返回的字符串作为 bytes 字符串返回 xpath : xpath 选择器 css : css 选择器 发送 post 请求 有时候想要请求数据的时候发送 post 请求，那么需要使用 Request 的子类 FormRequest 来实现， 如果想要爬虫一开始的时候就发送 post 请求，那么需要在爬虫类中重写 start_requests(self) 方法， 并且不在调用 start_urls 里的 url Update time： 2020-05-28 "},"Scrapy/Scrapy模拟人人网登录.html":{"url":"Scrapy/Scrapy模拟人人网登录.html","title":"Scrapy模拟人人网登录","keywords":"","body":"Scrapy模拟人人网登录 模拟人人网登录 发送 post 请求 有时候想要请求数据的时候发送 post 请求，那么需要使用 Request 的子类 FormRequest 来实现， 如果想要爬虫一开始的时候就发送 post 请求，那么需要在爬虫类中重写 start_requests(self) 方法， 并且不在调用 start_urls 里的 url # -*- coding: utf-8 -*- # renren.py import scrapy class RenrenSpider(scrapy.Spider): name = 'renren' allowed_domains = ['renren.com'] start_urls = ['http://renren.com/'] def start_requests(self): # 人人网登录的接口 url = \"http://www.renren.com/PLogin.do\" data = {\"email\": \"1315152****\", 'password': \"hu******\"} # 模拟登录 request = scrapy.FormRequest(url, formdata=data, callback=self.parse_page) yield request def parse_page(self, response): # 登录成功后访问个人主页 url = \"http://photo.renren.com/photo/972862448/albumlist/v7?offset=0&limit=40#\" respons = scrapy.Request(url, callback=self.parse_profile ) yield respons def parse_profile(self, response): # 将页面存储到本地 with open('dp.html', 'w', encoding='utf-8') as fp: fp.write(response.text) start_requests() 登录成功后 scrapy 会自动保存cookie 等信息。 Update time： 2020-05-28 "},"Scrapy/Scrapy模拟登录豆瓣网.html":{"url":"Scrapy/Scrapy模拟登录豆瓣网.html","title":"Scrapy模拟登录豆瓣网","keywords":"","body":"Scrapy模拟登录豆瓣网 # -*- coding: utf-8 -*- import scrapy from urllib import request from PIL import Image from urllib import request from base64 import b64encode import requests class DoubanSpider(scrapy.Spider): name = 'douban' allowed_domains = ['douban.com'] profile_url = 'https://www.douban.com/people/97956064/' login_url = 'https://accounts.douban.com/login' editsignature_url = 'https://www.douban.com/j/people/97956064/edit_signature' # 登录的 url start_urls = [\"https://accounts.douban.com/login\"] def parse(self, response): # 进入等率界面获取所需要的表单数据 # 登录需要的表单数据 formdata = { 'source': 'None', 'redir': 'https://www.douban.com/', 'form_email': '970138074@qq.com', 'form_password': 'pythonspider', 'remember': 'on', 'login': '登录' } # 获取验证码图片的 url captcha_url = response.css('img#captcha_image::attr(src)').get() # 判断是否需要验证码 if captcha_url: \"\"\" captcha-solution, captcha-id 都是动态变化的，需要临时获取 \"\"\" # 识别验证码 captcha = self.regonize_captcha(captcha_url) # 追加表单数据 formdata['captcha-solution'] = captcha captcha_id = response.xpath(\"//input[@name='captcha-id']/@value\").get() formdata['captcha-id'] = captcha_id yield scrapy.FormRequest(url=self.login_url, formdata=formdata, callback=self.parse_after_login) def parse_after_login(self, response): if response.url == 'https://www.douban.com/': yield scrapy.Request(self.profile_url, callback=self.parse_profile) print('登录成功！') else: print('登录失败！') def parse_profile(self, response): print(response.url) if response.url == self.profile_url: ck = response.xpath(\"//input[@name='ck']/@value\").get() formdata = { 'ck': ck, 'signature': '我可以自动识别图形验证码啦~~' } yield scrapy.FormRequest(self.editsignature_url, formdata=formdata, callback=self.parse_none) else: print('没有进入到个人中心') def parse_none(self,response): pass def regonize_captcha(self, image_url): captcha_url = image_url # 将验证码图片保存到本地 request.urlretrieve(captcha_url, 'captcha.png') # 阿里云识别验证码的 网站 recognize_url = 'http://jisuyzmsb.market.alicloudapi.com/captcha/recognize?type=e' formdata = {} with open('captcha.png', 'rb') as fp: data = fp.read() pic = b64encode(data) formdata['pic'] = pic appcode = '831a890b2cfe4ea0a8e345078434ebfc' headers = { 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Authorization': 'APPCODE ' + appcode } # 识别的结果，json格式的文件 response = requests.post(recognize_url, data=formdata, headers=headers) result = response.json() # 获取验证码 code = result['result']['code'] return code # 认为识别验证码 # def regonize_captcha(self,image_url): # request.urlretrieve(image_url, 'captcha.png') # image = Image.open('captcha.png') # image.show() # image.close() # captcha = input('请输入验证码：') # return captcha Update time： 2020-05-28 "},"Scrapy/Settings.html":{"url":"Scrapy/Settings.html","title":"Settings","keywords":"","body":"Settings Scrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。 设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。 设定可以通过下面介绍的多种机制进行设置。 设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。 ROBOTSTXT_OBEY 设置为False。默认是True。即遵守机器协议，那么在爬虫的时候，scrapy首先去找robots.txt文件，如果没有找到。则直接停止爬取。 DEFAULT_REQUEST_HEADERS 添加User-Agent。这个也是告诉服务器，我这个请求是一个正常的请求，不是一个爬虫。 # Override the default request headers: DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36' } ITEM_PIPELINES 启用一个Item Pipeline组件 ITEM_PIPELINES = { 'myproject.pipelines.PricePipeline': 300, 'myproject.pipelines.JsonWriterPipeline': 800, } 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。 DOWNLOAD_DELAY 默认: 0 下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度， 减轻服务器压力。同时也支持小数: DOWNLOAD_DELAY = 0.25 # 250 ms of delay 更多参考 Scrapy-Settings Update time： 2020-05-27 "},"Scrapy/使用Scrapy框架爬取糗事百科段子.html":{"url":"Scrapy/使用Scrapy框架爬取糗事百科段子.html","title":"使用Scrapy框架爬取糗事百科段子","keywords":"","body":"使用Scrapy框架爬取糗事百科段子 使用命令创建一个项目 scrapy startproject spider cd 到 该项目，创建一个爬虫 scrapy genspider qsbk \"www.qiushibaike.com\" 爬虫代码解析 # qsbk.py class QsbkSpider(scrapy.Spider): name = 'qsbk' allowed_domains = ['www.qiushibaike.com'] start_urls = ['https://www.qiushibaike.com/text/page/1/'] def parse(self, response): pass 要创建一个Spider，那么必须自定义一个类，继承自scrapy.Spider，然后在这个类中定义三个属性和一个方法。 name：这个爬虫的名字，名字必须是唯一的。 allow_domains：允许的域名。爬虫只会爬取这个域名下的网页，其他不是这个域名下的网页会被自动忽略。 start_urls：爬虫从这个变量中的url开始。 parse：引擎会把下载器下载回来的数据扔给爬虫解析，爬虫再把数据传给这个parse方法。这个是个固定的写法。这个方法的作用有两个，第一个是提取想要的数据。第二个是生成下一个请求的url。 将 start_urls 改为自己想要爬取的页面的url 修改settings.py代码 在做一个爬虫之前，一定要记得修改setttings.py中的设置。两个地方是强烈建议设置的。 ROBOTSTXT_OBEY设置为False。默认是True。即遵守机器协议，那么在爬虫的时候，scrapy首先去找robots.txt文件，如果没有找到。则直接停止爬取。 DEFAULT_REQUEST_HEADERS添加User-Agent。这个也是告诉服务器，我这个请求是一个正常的请求，不是一个爬虫。 完成的爬虫代码 爬虫部分代码： # -*- coding: utf-8 -*- import scrapy # 从根目录 导入 item 类 from spider.items import SpiderItem class QsbkSpider(scrapy.Spider): name = 'qsbk' allowed_domains = ['www.qiushibaike.com'] start_urls = ['https://www.qiushibaike.com/text/page/1/'] def parse(self, response): contents = response.xpath('//div[@class=\"article block untagged mb15 typs_hot\"]') print(\"*\" * 30) for duanzi in contents: # get() 获取一个内容 author = duanzi.xpath(\".//h2/text()\").get().strip() # getall() 获取所有内容，返回一个列表 content = duanzi.xpath('.//div[@class=\"content\"]//text()').getall() content = ' '.join(content).strip() # 另外一种传递数据的方式 创建一个 SpiderItem 对象 item = SpiderItem(author=author, content=content) ''' 或者使用这种方式 item = SpiderItem() item['author'] = author item['content'] = content ''' yield item # 将数据传到pipelines response 对象可以直执行 xpath语法来提取数据，如果想要获取里面的字符串，那么应该执行getall()或者get() 方法, getall() 方法获取所有文本，返回的是一个列表 get() 方法获取第一个，返回的是一个 str 类型 如果数据解析回来，要传给 pipeline 处理，那么可以使用 yield 来返回，或者收集所有的最后使用一个 return 来返回, items.py部分代码： import scrapy class SpiderItem(scrapy.Item): author = scrapy.Field() content = scrapy.Field() item 建议在“items.py” 中定义好， pipeline部分代码： import json class SpiderPipeline: def __init__(self): # 打开文件 或者在open_spider() 中打开文件 self.fp = open('duanzi.json','w',encoding='utf8') def open_spider(self,spider): pass def process_item(self, item, spider): # 采用 item 的方式 需要将其转化为字典 item_json = json.dumps(dict(item), ensure_ascii=False) self.fp.write(item_json +'\\n') return item def close_spider(self,spider): # 关闭文件 self.fp.close() 其中 def process_item(self, item, spider) 方法是必须有的，别的虽需要可以自己添加 优化 json 数据存储方式 (一) 导入 from scrapy.exporters import JsonItemExporter from scrapy.exporters import JsonItemExporter class SpiderPipeline: def __init__(self): # 以 wb 的方式打开文件 self.fp = open('duanzi.json', 'wb') # 创建对象 self.exporter = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf8') # 开始 self.exporter.start_exporting() def open_spider(self, spider): pass def process_item(self, item, spider): # 采用 item 的方式 需要将其转化为字典 #item_json = json.dumps(dict(item), ensure_ascii=False) # self.fp.write(item_json + '\\n') # 不再需要将 item 转化为字典 self.exporter.export_item(item) return item def close_spider(self, spider): self.exporter.finish_exporting() # 关闭文件 self.fp.close() 存储的结果 为一个列表，每个字典是列表的一项，当数据过大时，不推荐这种方式，因为其将整个 item z作为一项 导入文件的。 优化 json 数据存储方式 (二) 导入 from scrapy.exporters import JsonLinesItemExporter from scrapy.exporters import JsonLinesItemExporter class SpiderPipeline: def __init__(self): # 以 wb 的方式打开文件 self.fp = open('duanzi.json', 'wb') # 创建对象 self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf8') def open_spider(self, spider): pass def process_item(self, item, spider): self.exporter.export_item(item) return item def close_spider(self, spider): # 关闭文件 self.fp.close() 不需要开启和关闭，导入数据后的文件，仍然时一个字典一行 JsonItemExporter 和 JsonLinesItemExporter 保存数据的时候使用这两个类，让操作变的更简单 JsonItemExporter 每次将数据添加到内存中，最后统一写入到磁盘，好处时，存储的数据是一个满足 json 规则的数据，坏处是如果数据量比较大，那么内存消耗严重 JsonLinesItemExporter 每次调用 export_item 的时候就把这个item 存储到硬盘中，坏处是每一个字典是一行，整个文件不是满足json格式的文件，好处是每次处理数据的时候直接存储到硬盘，这样不会耗内存，数据也比较安全。 Update time： 2020-05-27 "},"Scrapy/糗事百科之抓取多个页面.html":{"url":"Scrapy/糗事百科之抓取多个页面.html","title":"糗事百科之抓取多个页面","keywords":"","body":"糗事百科之抓取多个页面 修改之前的 qsbk.py 文件 # -*- coding: utf-8 -*- import scrapy # 从根目录 导入 item 类 from spider.items import SpiderItem class QsbkSpider(scrapy.Spider): name = 'qsbk' allowed_domains = ['www.qiushibaike.com'] start_urls = ['https://www.qiushibaike.com/text/page/1/'] base_domain = \"https://www.qiushibaike.com\" def parse(self, response): contents = response.xpath('//div[@class=\"col1 old-style-col1\"]/div') for duanzi in contents: author = duanzi.xpath(\".//h2/text()\").get().strip() content = duanzi.xpath('.//div[@class=\"content\"]//text()').getall() content = ' '.join(content).strip() item = SpiderItem(author=author, content=content) yield item # 查找下页的 url # 查找最后一个 li 标签的 href 属性， # 若为空，表示当前页为最后一页，返回 # 否在, 请求下页的 url 获取内容 next_url = response.xpath('//div[@class=\"col1 old-style-col1\"]' '/ul/li[last()]/a/@href').get() if not next_url: return else: yield scrapy.Request(self.base_domain + next_url, callback=self.parse) 查找下页的 url 利用 scrapy.Request() 请求下一页，并利用回调函数进行解析html Update time： 2020-05-27 "},"Scrapy/CrawlSpider爬虫.html":{"url":"Scrapy/CrawlSpider爬虫.html","title":"CrawlSpider爬虫","keywords":"","body":"CrawlSpider爬虫 在上一个糗事百科的爬虫案例中。我们是自己在解析完整个页面后获取下一页的url，然后重新发送一个请求。有时候我们想要这样做，只要满足某个条件的 url，都给我进行爬取。那么这时候我们就可以通过CrawlSpider 来帮我们完成了。CrawlSpider 继承自 Spider，只不过是在之前的基础之上增加了新的功能，可以定义爬取的url的规则，以后scrapy碰到满足条件的url都进行爬取，而不用手动的yield Request。 创建CrawlSpider爬虫： 之前创建爬虫的方式是通过scrapy genspider [爬虫名字] [域名]的方式创建的。如果想要创建CrawlSpider爬虫，那么应该通过以下命令创建： scrapy genspider -t crawl [爬虫名字] [域名] scrapy genspider -t crawl wxapp 'wxapp-union.com' LinkExtractors链接提取器： 使用LinkExtractors可以不用程序员自己提取想要的url，然后发送请求。这些工作都可以交给LinkExtractors，他会在所有爬的页面中找到满足规则的url，实现自动的爬取。 class scrapy.linkextractors.LinkExtractor( allow = (), deny = (), allow_domains = (), deny_domains = (), deny_extensions = None, restrict_xpaths = (), tags = ('a','area'), attrs = ('href'), canonicalize = True, unique = True, process_value = None ) 主要参数讲解： allow：允许的url。所有满足这个正则表达式的url都会被提取。 deny：禁止的url。所有满足这个正则表达式的url都不会被提取。 allow_domains：允许的域名。只有在这个里面指定的域名的url才会被提取。 deny_domains：禁止的域名。所有在这个里面指定的域名的url都不会被提取。 restrict_xpaths：严格的xpath。和allow共同过滤链接。 Rule规则类： 定义爬虫的规则类。 class scrapy.spiders.Rule( link_extractor, callback = None, cb_kwargs = None, follow = None, process_links = None, process_request = None ) 主要参数讲解： link_extractor：一个LinkExtractor对象，用于定义爬取规则。 callback：满足这个规则的url，应该要执行哪个回调函数。因为CrawlSpider使用了parse作为回调函数，因此不要覆盖parse作为回调函数自己的回调函数。 follow：指定根据该规则从response中提取的链接是否需要跟进。 process_links：从link_extractor中获取到链接后会传递给这个函数，用来过滤不需要爬取的链接。 微信小程序社区CrawlSpider案例 初始的 爬虫文件 import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule class WxappSpiderSpider(CrawlSpider): name = 'wxapp_spider' allowed_domains = ['wxapp-union.com'] start_urls = [\"http://'wxapp-union.com'/\"] rules = ( Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) def parse_item(self, response): item = {} #item['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').get() #item['name'] = response.xpath('//div[@id=\"name\"]').get() #item['description'] = response.xpath('//div[@id=\"description\"]').get() return item 分析每页(教程栏)的url http://www.wxapp-union.com/portal.php?mod=list&catid=2&page=1 http://www.wxapp-union.com/portal.php?mod=list&catid=2&page=2 对于每一页 只有最后面的数字不同 分析每个详情页的 url http://www.wxapp-union.com/article-5985-1.html http://www.wxapp-union.com/article-6015-1.html http://www.wxapp-union.com/article-6002-1.html 只有中间的四个数字不同， 修改 rules rules = ( # 匹配每页的url Rule(LinkExtractor(allow=r'.+mod=list&catid=2&page=\\d'), # 允许的匹配规则，可以是正则匹配 follow=True), # 匹配详情页的 url Rule(LinkExtractor(allow=r'.+/article-.+\\.html'), callback=\"parse_detail\", # 解析详情页的回调函数，字符串格式的 follow=False # 详情页不跟进 ) ) CrawlSpider 需要使用 LinkExtractor 和Rule 决定爬虫的具体走向 allow 设置规则的方法：能够限制我们想得到的 url ， 不要根其他的 url 产生相同的正则表达式即可 什么情况下使用 follow : 如果在爬取页面的时候，需要满足当前的 url 再进行跟进，那么就设置为 True，否在设置为 False 什么情况下指定 callback ： 如果这个 url 对应的页面只是为了获取更多的 url, 并不需要页面的数据，那么可以不指定 callback, 如果想要获取 url 对应页面中的数据，那么就需要指定 callback 其他相关文件的设置和Spider 一样 Update time： 2020-05-28 "},"案例/":{"url":"案例/","title":"案例","keywords":"","body":"案例 Update time： 2020-07-19 "},"案例/爬取地理坐标.html":{"url":"案例/爬取地理坐标.html","title":"爬取地理坐标","keywords":"","body":"爬取地理坐标 爬取GPSspg查询网 import requests, re from urllib import parse def query(region): header = {'User-Agent': 'Opera/8.0 (Windows NT 5.1; U; en)'} url = 'http://apis.map.qq.com/jsapi?' data = { 'qt': 'poi', 'wd': region, 'pn': 0, 'rn': 10, 'rich_source': 'qipao', 'rich': 'web', 'nj': 0, 'c': 1, 'key': 'FBOBZ-VODWU-C7SVF-B2BDI-UK3JE-YBFUS', 'output': 'jsonp', 'pf': 'jsapi', 'ref': 'jsapi', 'cb': 'qq.maps._svcb3.search_service_0'} coordinate_url = url + parse.urlencode(data) r = requests.get(coordinate_url, headers=header) longitude = re.findall('\"pointx\":\\s*\"(.+?)\"', r.text)[0] latitude = re.findall('\"pointy\":\\s*\"(.+?)\"', r.text)[0] print([region, longitude, latitude]) query('佛山南海') # [‘佛山南海’, ‘113.142780’, ‘23.028820’] 爬取百度地图拾取坐标系统 from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import re, pandas as pd def coordinate(site): # 创建浏览器驱动对象 driver = webdriver.Firefox() driver.get('http://api.map.baidu.com/lbsapi/getpoint/index.html') # 显式等待，设置timeout wait = WebDriverWait(driver, 9) # 判断输入框是否加载 input = wait.until( EC.presence_of_element_located( (By.CSS_SELECTOR, '#localvalue'))) # 判断搜索按钮是否加载 submit = wait.until( EC.element_to_be_clickable( (By.CSS_SELECTOR, '#localsearch'))) # 输入搜索词，点击搜索按钮 input.clear() input.send_keys(site) submit.click() # 等待坐标 wait.until( EC.presence_of_element_located( (By.CSS_SELECTOR, '#no_0'))) # 获取网页文本，提取经纬度 source = driver.page_source xy = re.findall('坐标：([\\d.]+),([\\d.]+)', source) # 转浮点数，取中位数 df = pd.DataFrame(xy, columns=['longitude', 'latitude']) df['longitude'] = pd.to_numeric(df['longitude']) df['latitude'] = pd.to_numeric(df['latitude']) longitude = df['longitude'].median() latitude = df['latitude'].median() # 关闭浏览器驱动 driver.close() return [longitude, latitude] print(coordinate('南海桂城地铁站')) # [113.1611575, 23.044811000000003] 在使用 webdriver 的时候，需要将下载的驱动器加载到 PATH 环境变量中，另一种方案：将驱动器放到当前脚本目录下。 参考 转自Python 爬取经纬度 Python 爬取经纬度 获取GPS / 经纬度转换 百度地图：拾取坐标系统 Update time： 2020-07-19 "},"案例/批量查询地址经纬度信息.html":{"url":"案例/批量查询地址经纬度信息.html","title":"批量查询地址经纬度信息","keywords":"","body":"批量查询地址经纬度信息 抓包 打开https://jingweidu.51240.com/ F12键(mac电脑快捷键option+command+I)打开开发者工具Network面板 搜索框输入查询地址，回车查询 开发者工具Network会看到截图中的网址 点击Preview，能看到具体信息 经纬度查询爬虫函数query 根据抓包分析，构造网址模板template 使用requests库发起访问 使用正则表达式re库解析出经纬度信息 import requests def query(addr): #查询addr的经纬度 template = 'https://apis.map.qq.com/jsapi?qt=geoc&addr={addr}&key=UGMBZ-CINWR-DDRW5-W52AK-D3ENK-ZEBRC&output=jsonp&pf=jsapi&ref=jsapi&cb=qq.maps._svcb2.geocoder0' url = template.format(addr=addr) resp = requests.get(url) x = re.findall('pointx\":\"(.*?)\",',resp.text)[0] y = re.findall('pointy\":\"(.*?)\",',resp.text)[0] return x,y query(addr=\"南京师范大学\") resp = requests.get(url) 的返回值： qq.maps._svcb2.geocoder0&&qq.maps._svcb2.geocoder0({ \"info\":{ \"type\":45, \"error\":0, \"time\":0 }, \"detail\":{ \"name\":\"南京师范大学(中北学院)\", \"city\":\"南京市\", \"district\":\"栖霞区\", \"adcode\":\"320113\", \"pointx\":\"118.909073\", \"pointy\":\"32.115414\", \"gps_type\":\"11\", \"reliability\":\"7\", \"province\":\"江苏省\", \"deviation\":\"1000.000000\", \"pcd_conflict_flag\":\"0\", \"query_status\":\"0\", \"server_retcode\":\"0\", \"similarity\":\"0.800000\" , \"split_addr\":\"\" , \"street\":\"\" , \"street_number\":\"\", \"key_poi\":\"南京师范大学\", \"category_code\":\"\", \"address_type\":\"1\" , \"poi_id\":\"1787570519875429111\", \"town\":\"仙林街道\", \"town_code\":\"320113007\", \"key_role\":\"\" } }) 通过正则表达式查询获取最后的经纬度信息。 ('118.909073', '32.115414') 测试数据 import pandas as pd df = pd.read_csv(\"test.csv\") df df['addr'] 0 山东省潍坊市安丘市兴安街道 1 浙江省杭州市萧山区 2 广东省广州市番禺区 3 陕西省西安市莲湖区 Name: addr, dtype: object 批量查询 使用apply方法调用query函数批量查询经纬度 df['addr'].apply(query) 0 (119.161423, 36.331699) 1 (120.264570, 30.185340) 2 (113.384240, 22.937720) 3 (108.940200, 34.267030) Name: addr, dtype: object 保存 df['经纬度']=df['addr'].apply(query) df 导出csv 结果导出到 csv 中 df.to_csv('result.csv') 参考 爬虫小案例 | 批量查询地址经纬度信息 Update time： 2020-08-10 "},"案例/拉钩网数据爬取.html":{"url":"案例/拉钩网数据爬取.html","title":"拉钩网数据爬取","keywords":"","body":"拉钩网数据爬取 特别声明 众所周知，拉勾网的反爬机制更新较为频繁，本博客所分享的方法不一定永久有效，即具有时效性。研究该网站的反爬机制仅供交流学习，千万不要用于做违法的事情。 网站介绍 拉勾网是一个专业的互联网招聘平台，该网站上有很多公司的招聘信息，所以该网站也成了很多爬虫爬取的重点对象。 思路分析 本文以Python为例进行思路分析，首先，在搜索框中输入关键字Python，并点击搜索 可以跳转到如上图所示的页面，我们要爬取的数据就是下面的职位信息 查看网页源代码，我们可以发现并没有我们想要的数据，由此可以判断数据通过其他的URL请求得到，下面进行抓包分析 通过抓包，我们发现了请求的URL，请求方式为POST，但是，当我们直接用POST方式请求时，却不能获取到数据，直接给你返回“请求太频繁”（这个提示应该是假的），我尝试增加请求头中的信息，依旧没有用。后来，经过分析我知道了网站后台可能会判断你请求时的cookie，只有cookie正确，才给你返回信息，因此，我们的思路就很明确了，只要在请求之前拿到cookie，并且在请求时在加上cookie就行了。 核心思路： 为了能够拿到我们想要的职位信息，我们分两步走。 第一步，先用get的方式请求一开始的URL，即我们访问页面的URL，这一步的目的是为了能够得到cookie，为下一步请求做准备。 第一次请求的URL：https://www.lagou.com/jobs/list_Python?labelWords=&fromSearch=true&suginput= 第二步，我们用第一步拿到的cookie来发送POST请求，请求的URL为： https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false 另外作为 POST 请求，在请求数据的时候，可以传递查询的参数和数据 具体实现 首先，我们可以用requests库中的session方法来创建一个session对象来存储cookie，然后再用这个cookie进行第二步中的请求即可。 代码 \"\"\" 拉勾网反爬机制分析： 通过两次请求来获取职位列表， 第一次请求原始页面获取cookie 第二次请求时利用第一次获取到的cookie \"\"\" import requests # 第一次请求的URL first_url = 'https://www.lagou.com/jobs/list_Python?labelWords=&fromSearch=true&suginput=' # 第二次请求的URL second_url = 'https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false' # 伪装请求头 headers = { 'Accept': 'application/json, text/javascript, */*; q=0.01', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'Content-Length': '25', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Host': 'www.lagou.com', 'Origin': 'https://www.lagou.com', 'Referer': 'https://www.lagou.com/jobs/list_Python?labelWords=&fromSearch=true&suginput=', 'Sec-Fetch-Mode': 'cors', 'Sec-Fetch-Site': 'same-origin', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36', 'X-Anit-Forge-Code': '0', 'X-Anit-Forge-Token': 'None', 'X-Requested-With': 'XMLHttpRequest' } # 创建一个session对象 session = requests.session() # 请求的数据 data = { 'first': 'true', 'pn': '1', 'kd': 'Python' } session.get(first_url, headers={ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36' }) result = session.post(second_url, headers=headers, data=data, allow_redirects=False) print(result.json()) 分页爬取： # -*- encoding: utf-8 -*- import requests import json import jsonpath import time import pandas as pd # 第一次请求的URL first_url = 'https://www.lagou.com/jobs/list_Python?labelWords=&fromSearch=true&suginput=' # 第二次请求的URL second_url = 'https://www.lagou.com/jobs/positionAjax.json?' # 伪装请求头 headers = { 'Accept': 'application/json, text/javascript, */*; q=0.01', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'Content-Length': '25', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Host': 'www.lagou.com', 'Origin': 'https://www.lagou.com', 'Referer': 'https://www.lagou.com/jobs/list_Python?labelWords=&fromSearch=true&suginput=', 'Sec-Fetch-Mode': 'cors', 'Sec-Fetch-Site': 'same-origin', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36', 'X-Anit-Forge-Code': '0', 'X-Anit-Forge-Token': 'None', 'X-Requested-With': 'XMLHttpRequest' } # 创建一个session对象 session = requests.session() session.get(first_url, headers={ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36' }) # 可以自己改变城市 params = { 'px': 'default', 'city': '北京', 'needAddtionalResult': 'false' } page = 1 # 爬取 10 页 for i in range(10): # 请求的数据 form_data = { 'first': 'false', 'pn': page, 'kd': 'python' } response = session.post(url=second_url , headers=headers , params=params , data=form_data, allow_redirects=False) python_obj = json.loads(response.content) result_list = jsonpath.jsonpath(python_obj, '$..result')[0] for item in result_list: po = dict() po[u'positionName'] = item[\"positionName\"] po[u'city'] = item['city'] po[u'district'] = item['district'] po[u'skillLables'] = item['skillLables'] po[u'salary'] = item['salary'] po[u'jobNature'] = item['jobNature'] po[u'education'] = item['education'] po[u'companyFullName'] = item['companyFullName'] po[u'companySize'] = item['companySize'] po[u'createTime'] = item['createTime'] po = pd.DataFrame(po) po.to_csv('LGW.csv', header=False, index=False, mode='a+') page += 1 time.sleep(4) 参考 拉勾网反爬机制分析（2020年1月21日） Update time： 2020-08-13 "},"案例/selenium静态数据爬取.html":{"url":"案例/selenium静态数据爬取.html","title":"selenium静态数据爬取","keywords":"","body":"selenium静态数据爬取 网站：http://www.cmakjgl.cn/cglist.aspx 当我们切换不同页时，可以发现地址栏里的 url 是没有发生变化的，对于这样的网站我们可以分析数据接口 但传递的参数是进行加密的，无法正确解析所传递参数的含义，这是我们可以借助 selenium 进行数据的爬取； 代码： import requests import pandas as pd import csv from selenium import webdriver from selenium.webdriver.common.keys import Keys from lxml import etree import time from selenium.webdriver.chrome.options import Options chrome_options=Options() #设置chrome浏览器无界面模式 chrome_options.add_argument('--headless') # 启动浏览器驱动 driver = webdriver.Chrome(chrome_options=chrome_options) url = 'http://www.cmakjgl.cn/cglist.aspx' driver.get(url) # 停止 2 秒，留足时间 time.sleep(2) # 窗口最大化 # driver.maximize_window() # 循环的进行爬取 for ii in range(186): # 寻找网页元素 element = driver.find_element_by_class_name('dg') tr_lists = element.find_elements_by_tag_name('tr') # 获取页面的句柄值 window_2 = driver.current_window_handle for i in range(len(tr_lists)-3): # 定义空列表 data1 = [] # 记录是从当前表格的第三(i+2)行开始的,然后找到当前行的所有单元格 td_lists = tr_lists[i+2].find_elements_by_tag_name('td') # 获取单元格的内容 for td_list in td_lists: data1.append(td_list.text) # 获取表格的超链接 # 当前表格共 100个这样的链接，每行有两个，我们需要点击第二个，打开新的页面 abstract_buttons = driver.find_elements_by_xpath('//a[contains(@href,\"cgview.aspx?id=\")]') abstract_buttons[0+i*2].click() print('abstract_page ' + str(i+1)+ \" has done\") # 等待三秒, 加载新的网页 time.sleep(3) # 获取当前窗口的句柄 window_1 = driver.current_window_handle # 获取所有窗口的句柄值 hand4 = driver.window_handles # 获取所有窗口的句柄值, 并切换至当前窗口 for current_window in hand4: if current_window != window_1: driver.switch_to.window(current_window) # 获取摘要信息，并显示 abstract = driver.find_element_by_xpath('//span[@id=\"ctl00_ContentPlaceHolder1_lblACHIVE_BRIEF\"]') data1.append(abstract.text) # 关闭摘要信息窗口 driver.close() # 切换回原窗口 driver.switch_to.window(window_2) data2 = pd.DataFrame(data1).T data2.to_csv('qixiang0.csv',header=False,index=False,mode='a+') print('page '+ str(ii+1) +' has done') next_page_button = driver.find_element_by_xpath('//a[@id=\"ctl00_ContentPlaceHolder1_gdvList_ctl01_lnkNext\"]') next_page_button.click() 程序的关键是模拟点击行为后，对网页进行循环的爬取。 参考 B站 ：龙王山小青椒 python爬虫-成名绝技selenium-所见即所得 Update time： 2020-08-12 "},"案例/爬取淘宝商品评论.html":{"url":"案例/爬取淘宝商品评论.html","title":"爬取淘宝商品评论","keywords":"","body":"爬取淘宝商品评论 流程 这里，我们以一个随机商品为例，流程如下： 根据商品详情页链接获取真实的评论请求url 请求评论 url，接收响应 解析数据，获取评论总数和评论数据 存储数据到本地 根据评论总数构造循环翻页 淘宝的评论是通过 JS 动态渲染出来的，并没有在初始请求的网页源码中，所以我们要找到发送新请求的 url，这个并不难找。 右键 f12 进入检查，此时没有发送评论请求，评论并没有加载；当点击网页的评论按钮时，有新的请求被发送了，“feedRateList”开头的新请求就是我们要找的。从preview中可以看出，这是一个json，里面包含了评论和其它的数据。这里可以把整个json拿出来，但里面有很多其它keys，很多我并不知道含义，所以我只提取了自己感兴趣的数据。 分析完毕，开始爬虫： 获取评论的 url: 我们只需要获取 Request URL 的前面一部分。 页面的 url https://rate.taobao.com/feedRateList.htm? 问号后面的部分，我们可以作为参数进行传递； params = { 'auctionNumId': 616727555136, #商品的 DI 'currentPageNum': 1 # 评论的页数，通过改变这个参数值，我们可以获取多页的评论 } 设置 请求头： headers = { # 从哪个页面发出的数据申请，每个网站可能略有不同 'referer':'https://item.taobao.com/item.htm?id=616727555136&ali_trackid=2:mm_12238993_43806065_714972723:1597052770_255_1467045357&spm=a231o.7712113%2Fg.1004.35&pvid=200_11.27.93.104_284878_1597052761050&bxsign=tbk159705277066145c815046b62c30a7b16b4c95524bc36', # 用的哪个浏览器 \"user-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', # 用户的账号、密码等信息，主要用来反爬虫，用户登录后可以获取自己的cookie数据 'cookie':\"cna=b/D8FYOTMVUCATrwJ8wV3AAV; thw=cn; hng=CN%7Czh-CN%7CCNY%7C156; miid=1525554089483641014; tracknick=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; tg=0; _samesite_flag_=true; cookie2=1b33c97adb02c7a77f9b89cfd5b171c4; t=0189b94436cd684bd44103b442e690f0; _tb_token_=38113a95557b8; v=0; UM_distinctid=173d7c2723922c-03c43fa2475c66-3323765-144000-173d7c2723a615; sgcookie=E2HGXmd4MlsLL6HyGk0KJ; uc3=vt3=F8dBxG2m6DG9ZRDBCCM%3D&id2=UUjXbEBZkN7mJw%3D%3D&lg2=Vq8l%2BKCLz3%2F65A%3D%3D&nk2=1TxKfiqLL2jq2i9bmfnbrTmayVA%3D; csg=84f3fcd6; lgc=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; dnk=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; skt=dfc81f3d58f5b26e; existShop=MTU5NzA1MjczMQ%3D%3D; uc4=id4=0%40U2o1ZOW2sQbQakAQ5T12awSrkm3p&nk4=0%401%2BCc9B0f4Q5M%2F%2BMtI%2FU07r71QIM2V4fqLr0KgI3bXg%3D%3D; _cc_=W5iHLLyFfA%3D%3D; lLtC1_=1; enc=4tzEh0tXDSIEBX1SFap7WowBYIv7XkkJzvG9nbq9RshD9LloBqEnF9xLJhJiDTYADhWncQtfr4F5XQH1KsK%2FGQ%3D%3D; mt=ci=-1_0; uc1=cookie14=UoTV6ymGDU%2BQ%2FQ%3D%3D&pas=0&existShop=false&cookie21=W5iHLLyFeYZ1WM9hVnmS&cookie15=URm48syIIVrSKA%3D%3D&cookie16=U%2BGCWk%2F74Mx5tgzv3dWpnhjPaQ%3D%3D; tfstk=cX-VBuGytmn4JcjsJisacWBGduEAatCGOuWFoehGk99TM-IVTsq_6TcYHTWN90bc.; l=eBSQWiJnqBSbB8NFBO5Zourza77tBIRb8sPzaNbMiInca6tP6UIYhNQqlbfkJdtjgtfAoeKPT3lYXREJ8c4LRETjGO0qOC0eQxv9-; isg=BFhY8mH9swr4n50_CEdVCJGAKYbqQbzL7c1_CJJJURNkLfkXO1AXW6NLZWUdPXSjcna=b/D8FYOTMVUCATrwJ8wV3AAV; thw=cn; hng=CN%7Czh-CN%7CCNY%7C156; miid=1525554089483641014; tracknick=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; tg=0; _samesite_flag_=true; cookie2=1b33c97adb02c7a77f9b89cfd5b171c4; t=0189b94436cd684bd44103b442e690f0; _tb_token_=38113a95557b8; v=0; UM_distinctid=173d7c2723922c-03c43fa2475c66-3323765-144000-173d7c2723a615; sgcookie=E2HGXmd4MlsLL6HyGk0KJ; uc3=vt3=F8dBxG2m6DG9ZRDBCCM%3D&id2=UUjXbEBZkN7mJw%3D%3D&lg2=Vq8l%2BKCLz3%2F65A%3D%3D&nk2=1TxKfiqLL2jq2i9bmfnbrTmayVA%3D; csg=84f3fcd6; lgc=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; dnk=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; skt=dfc81f3d58f5b26e; existShop=MTU5NzA1MjczMQ%3D%3D; uc4=id4=0%40U2o1ZOW2sQbQakAQ5T12awSrkm3p&nk4=0%401%2BCc9B0f4Q5M%2F%2BMtI%2FU07r71QIM2V4fqLr0KgI3bXg%3D%3D; _cc_=W5iHLLyFfA%3D%3D; lLtC1_=1; enc=4tzEh0tXDSIEBX1SFap7WowBYIv7XkkJzvG9nbq9RshD9LloBqEnF9xLJhJiDTYADhWncQtfr4F5XQH1KsK%2FGQ%3D%3D; mt=ci=-1_0; uc1=cookie14=UoTV6ymGDU%2BQ%2FQ%3D%3D&pas=0&existShop=false&cookie21=W5iHLLyFeYZ1WM9hVnmS&cookie15=URm48syIIVrSKA%3D%3D&cookie16=U%2BGCWk%2F74Mx5tgzv3dWpnhjPaQ%3D%3D; tfstk=cX-VBuGytmn4JcjsJisacWBGduEAatCGOuWFoehGk99TM-IVTsq_6TcYHTWN90bc.; l=eBSQWiJnqBSbB8NFBO5Zourza77tBIRb8sPzaNbMiInca6tP6UIYhNQqlbfkJdtjgtfAoeKPT3lYXREJ8c4LRETjGO0qOC0eQxv9-; isg=BFhY8mH9swr4n50_CEdVCJGAKYbqQbzL7c1_CJJJURNkLfkXO1AXW6NLZWUdPXSj\" } 看一下效果： import requests import re url = 'https://rate.taobao.com/feedRateList.htm?' headers = { 'referer':'https://item.taobao.com/item.htm?id=616727555136&ali_trackid=2:mm_12238993_43806065_714972723:1597052770_255_1467045357&spm=a231o.7712113%2Fg.1004.35&pvid=200_11.27.93.104_284878_1597052761050&bxsign=tbk159705277066145c815046b62c30a7b16b4c95524bc36', \"user-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'cookie':\"cna=b/D8FYOTMVUCATrwJ8wV3AAV; thw=cn; hng=CN%7Czh-CN%7CCNY%7C156; miid=1525554089483641014; tracknick=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; tg=0; _samesite_flag_=true; cookie2=1b33c97adb02c7a77f9b89cfd5b171c4; t=0189b94436cd684bd44103b442e690f0; _tb_token_=38113a95557b8; v=0; UM_distinctid=173d7c2723922c-03c43fa2475c66-3323765-144000-173d7c2723a615; sgcookie=E2HGXmd4MlsLL6HyGk0KJ; uc3=vt3=F8dBxG2m6DG9ZRDBCCM%3D&id2=UUjXbEBZkN7mJw%3D%3D&lg2=Vq8l%2BKCLz3%2F65A%3D%3D&nk2=1TxKfiqLL2jq2i9bmfnbrTmayVA%3D; csg=84f3fcd6; lgc=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; dnk=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; skt=dfc81f3d58f5b26e; existShop=MTU5NzA1MjczMQ%3D%3D; uc4=id4=0%40U2o1ZOW2sQbQakAQ5T12awSrkm3p&nk4=0%401%2BCc9B0f4Q5M%2F%2BMtI%2FU07r71QIM2V4fqLr0KgI3bXg%3D%3D; _cc_=W5iHLLyFfA%3D%3D; lLtC1_=1; enc=4tzEh0tXDSIEBX1SFap7WowBYIv7XkkJzvG9nbq9RshD9LloBqEnF9xLJhJiDTYADhWncQtfr4F5XQH1KsK%2FGQ%3D%3D; mt=ci=-1_0; uc1=cookie14=UoTV6ymGDU%2BQ%2FQ%3D%3D&pas=0&existShop=false&cookie21=W5iHLLyFeYZ1WM9hVnmS&cookie15=URm48syIIVrSKA%3D%3D&cookie16=U%2BGCWk%2F74Mx5tgzv3dWpnhjPaQ%3D%3D; tfstk=cX-VBuGytmn4JcjsJisacWBGduEAatCGOuWFoehGk99TM-IVTsq_6TcYHTWN90bc.; l=eBSQWiJnqBSbB8NFBO5Zourza77tBIRb8sPzaNbMiInca6tP6UIYhNQqlbfkJdtjgtfAoeKPT3lYXREJ8c4LRETjGO0qOC0eQxv9-; isg=BFhY8mH9swr4n50_CEdVCJGAKYbqQbzL7c1_CJJJURNkLfkXO1AXW6NLZWUdPXSjcna=b/D8FYOTMVUCATrwJ8wV3AAV; thw=cn; hng=CN%7Czh-CN%7CCNY%7C156; miid=1525554089483641014; tracknick=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; tg=0; _samesite_flag_=true; cookie2=1b33c97adb02c7a77f9b89cfd5b171c4; t=0189b94436cd684bd44103b442e690f0; _tb_token_=38113a95557b8; v=0; UM_distinctid=173d7c2723922c-03c43fa2475c66-3323765-144000-173d7c2723a615; sgcookie=E2HGXmd4MlsLL6HyGk0KJ; uc3=vt3=F8dBxG2m6DG9ZRDBCCM%3D&id2=UUjXbEBZkN7mJw%3D%3D&lg2=Vq8l%2BKCLz3%2F65A%3D%3D&nk2=1TxKfiqLL2jq2i9bmfnbrTmayVA%3D; csg=84f3fcd6; lgc=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; dnk=%5Cu8BFB%5Cu4F60%5Cu7684%5Cu611F%5Cu89C9%5Cu50CF%5Cu6625%5Cu5929%5Cu5684%5Cu5416; skt=dfc81f3d58f5b26e; existShop=MTU5NzA1MjczMQ%3D%3D; uc4=id4=0%40U2o1ZOW2sQbQakAQ5T12awSrkm3p&nk4=0%401%2BCc9B0f4Q5M%2F%2BMtI%2FU07r71QIM2V4fqLr0KgI3bXg%3D%3D; _cc_=W5iHLLyFfA%3D%3D; lLtC1_=1; enc=4tzEh0tXDSIEBX1SFap7WowBYIv7XkkJzvG9nbq9RshD9LloBqEnF9xLJhJiDTYADhWncQtfr4F5XQH1KsK%2FGQ%3D%3D; mt=ci=-1_0; uc1=cookie14=UoTV6ymGDU%2BQ%2FQ%3D%3D&pas=0&existShop=false&cookie21=W5iHLLyFeYZ1WM9hVnmS&cookie15=URm48syIIVrSKA%3D%3D&cookie16=U%2BGCWk%2F74Mx5tgzv3dWpnhjPaQ%3D%3D; tfstk=cX-VBuGytmn4JcjsJisacWBGduEAatCGOuWFoehGk99TM-IVTsq_6TcYHTWN90bc.; l=eBSQWiJnqBSbB8NFBO5Zourza77tBIRb8sPzaNbMiInca6tP6UIYhNQqlbfkJdtjgtfAoeKPT3lYXREJ8c4LRETjGO0qOC0eQxv9-; isg=BFhY8mH9swr4n50_CEdVCJGAKYbqQbzL7c1_CJJJURNkLfkXO1AXW6NLZWUdPXSj\" } params = { 'auctionNumId': 616727555136, 'currentPageNum': 1 } page = requests.get(url,params=params, headers=headers).text 将开头的 \\r\\n 替换掉后，可以其转化为 json 格式的文件。 page = re.sub(r'[\\n\\r()]', '', page) page=json.loads(content) # 或者一步到位 # page=json.loads(re.sub(r'[\\n\\r()]', '', page)) 转化后的文件内容为 json 格式的文件； 如果这里不将获取到的页面转为 json 格式的文件，我们可以用正则表达式获取我们所需要的内容。 查看原网页我们可以知道，商品的评论在 comments 下的 content 对应的内容； 查看 都有哪些关键字： page.keys() # dict_keys(['qnaDisabled', 'watershed', 'search', 'total', 'comments', 'currentPageNum', 'maxPage']) 我们可以利用关键字获取我们所需的参数： # 总共有多少评论 page['total'] # 用户的评论，为一个字典组成的列表 page['comments'] len(page['comments']) # 当前页多少评论 # 20 # 获取每一条评论 for item in page['comments']: print(item['content']) 这部分只是获取了用户的评论内容。同样的我们也可以获取其它内容。 参考： python爬虫之三 —— 淘宝评论 【淘宝爬虫2】淘宝评论爬取-新手入门级Python爬虫实战 爬虫教程 Update time： 2020-08-11 "},"案例/爬取京东商品评论.html":{"url":"案例/爬取京东商品评论.html","title":"爬取京东商品评论","keywords":"","body":"爬取京东商品评论 打开京东主页，找到需要爬取评论的商品： 右键 检查网页，将记载的文件先清空，找到商品评论处 ，当切换不同页的评论时，可以看到加载出新的文件： 将 Request URL 复制到浏览器可以看到本页评论的内容； https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98&productId=100005207111&score=0&sortType=5&page=0&pageSize=10&isShadowSku=0&rid=0&fold=1 url 分析：https://club.jd.com/comment/productPageComments.action? 后面的部分是请求所需的参数，在下面的 Query String Parameters 中也可以看到。在发送请求的时候 将 https://club.jd.com/comment/productPageComments.action? 作为 url, 后面的部分作为参数进行传递。 参数解析： callback: 固定参数 productId: 商品 ID score: 不同的评价类型, 0 : 全部评价 1：差评 2：中评 3：好评 sortType: 评论排序方式：推荐排序(5)，时间排序(6) page: 评论页，通过改变评论页，可以实现不同页面评论的循环爬取 pageSize: 固定参数 isShadowSku: rid: fold: 测试： import requests import json import re import pandas as pd import time url = 'https://club.jd.com/comment/productPageComments.action?' for i in range(50): # 请求参数 params = { 'callback': 'fetchJSON_comment98', # 固定值 'productId': '100005207111', # 产品 ID 'score': '0', # 0-5 分别对应不同的评价，好评、中评、全部评价等 'sortType': '5', # 排序方式 'page': i, # 当前的评论页 'pageSize': '10', # 每页评论的数量 'isShadowSku': '0', 'rid': '0', 'fold': '1' } headers = { 'Referer': 'https://item.jd.com/100005207111.html', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'Cookie': '__jdu=1568007721111834248469; shshshfpa=6814de79-d912-fb65-2445-e0c1001418be-1568007725; shshshfpb=eK6h%2FTTG3UOQIjI3nubc4rg%3D%3D; pinId=uKm5zMomrBzAmLNKou2uKDqrJX175ugD; unpl=V2_ZzNtbRcFShclDkVTchwOAGIDR1VKV0EWcwxCBnobD1IzURFaclRCFnQUR11nGF4UZgsZX0RcQRNFCEVkexhdBGAAE19BVXMlRQtGZHopXAFgChNcRFFAFXUIRl15HF8AbgYVVXJnRBV8OHYfIl9VAmIAE1hAVEcldg1BUX8bbARXARNcQVBKFXwJRmQwd11IZwcVVENWRRN2CEZUexBeAGQGG1hFX3MVdAlHVXseXQZmMxE%3d; __jdv=122270672|google|tw6|cpc|not set|1596435399183; areaId=12; ipLoc-djd=12-904-3373-0; PCSYCityID=CN_320000_320100_0; __jdc=122270672; __jda=122270672.1568007721111834248469.1568007721.1596435399.1597198601.61; shshshfp=220f18c9406df7935c99c0b11d700617; 3AB9D23F7A4B3C9B=IXUELWJK3WKMGCI6K6VNEMY5JTJRYFIN7UACHEU53PFRGL26M3Z4EPHTURYSBFSZOFLSHWLCALKBM2HL2Z6PVCSWZ4; jwotest_product=99; wlfstk_smdl=ehul02b6y6mrpp3c0io9rgen0mvqvz09; TrackID=1odMbOfz9qDnsQUriX0MI0xuEBMGOvndQJls-Cnyljpewp5cEF-8coPrSiWOAgRyjr8am57HCcwV9xQf_rr3xTPy5hoA-MhDYUFaH7zOoUlO458mlGY3vk0rpwZPB_jrZ; thor=E720FC5049F725A474B2F674576F76565C554ECB4F3557F926E8468A57A9F7319EE2F9D0A1DB99CB745E9A3DD4FB61D813FCC037D93474C5510BE4250A6DA7A54A946F6619D1239AE6A096341CF9A38620295216FAF185554394D80EC04D8B5A7B46EF7FC4618FA645AD46ED4BD7383928B3013693A6F5931E4F5AA7EFCBBA78; pin=%E4%B8%B6%E5%8C%97%E5%9F%8E%E9%B1%BC%E5%AE%89%E7%94%9F; unick=___%E6%A2%A6%E5%AF%90___; ceshi3.com=000; _tp=6KbAcOO79y3rwvtuJvHoHRBSFSbWIqCOxKNAM%2FSmxbwMBsbJ8KsKQWmVHGQC11rU1S2%2FwNGWzgqqKzflVhJMlw%3D%3D; _pst=%E4%B8%B6%E5%8C%97%E5%9F%8E%E9%B1%BC%E5%AE%89%E7%94%9F; shshshsID=8ed6a639183b7cc4d65115ddd4ec2c4b_5_1597199326879; __jdb=122270672.8.1568007721111834248469|61.1597198601; JSESSIONID=C4161CA90DF9781E4CF679DB423DF2D6.s1' } res = requests.get(url=url,params=params,headers=headers) print(res.status_code) # 返回的内容并不是规范的json 格式文件, 利用正则表达式将其转化为规范的 json 文件 html_json = re.search('(? 上面是商品的全部评价，当然也可以只爬取当前商品的评价； 刷新网页，检查，清空。 选择只看当前商品评价： 我们同样可以 发送Request URL 的 request 请求，并传递相应的参数获取评论。 测试： # -*- encoding: utf-8 -*- \"\"\" @software: PyCharm @file : jd2.py @time : 2020/8/12 \"\"\" import requests import json import re import pandas as pd import time url = 'https://club.jd.com/comment/skuProductPageComments.action?' for i in range(25): # 请求参数, 与全部评价相比少了一个 'rid' 参数 params = { 'callback': 'fetchJSON_comment98', # 固定值 'productId': '100005207111', # 产品 ID 'score': '0', # 0-5 分别对应不同的评价，好评、中评、全部评价等 'sortType': '5', # 排序方式 'page': i, # 当前的评论页 'pageSize': '10', # 每页评论的数量 'isShadowSku': '0', 'fold': '1' } headers = { 'Referer': 'https://item.jd.com/100005207111.html', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'Cookie': '__jdu=1568007721111834248469; shshshfpa=6814de79-d912-fb65-2445-e0c1001418be-1568007725; shshshfpb=eK6h%2FTTG3UOQIjI3nubc4rg%3D%3D; pinId=uKm5zMomrBzAmLNKou2uKDqrJX175ugD; unpl=V2_ZzNtbRcFShclDkVTchwOAGIDR1VKV0EWcwxCBnobD1IzURFaclRCFnQUR11nGF4UZgsZX0RcQRNFCEVkexhdBGAAE19BVXMlRQtGZHopXAFgChNcRFFAFXUIRl15HF8AbgYVVXJnRBV8OHYfIl9VAmIAE1hAVEcldg1BUX8bbARXARNcQVBKFXwJRmQwd11IZwcVVENWRRN2CEZUexBeAGQGG1hFX3MVdAlHVXseXQZmMxE%3d; __jdv=122270672|google|tw6|cpc|not set|1596435399183; areaId=12; ipLoc-djd=12-904-3373-0; PCSYCityID=CN_320000_320100_0; __jdc=122270672; __jda=122270672.1568007721111834248469.1568007721.1596435399.1597198601.61; shshshfp=220f18c9406df7935c99c0b11d700617; 3AB9D23F7A4B3C9B=IXUELWJK3WKMGCI6K6VNEMY5JTJRYFIN7UACHEU53PFRGL26M3Z4EPHTURYSBFSZOFLSHWLCALKBM2HL2Z6PVCSWZ4; jwotest_product=99; wlfstk_smdl=ehul02b6y6mrpp3c0io9rgen0mvqvz09; TrackID=1odMbOfz9qDnsQUriX0MI0xuEBMGOvndQJls-Cnyljpewp5cEF-8coPrSiWOAgRyjr8am57HCcwV9xQf_rr3xTPy5hoA-MhDYUFaH7zOoUlO458mlGY3vk0rpwZPB_jrZ; thor=E720FC5049F725A474B2F674576F76565C554ECB4F3557F926E8468A57A9F7319EE2F9D0A1DB99CB745E9A3DD4FB61D813FCC037D93474C5510BE4250A6DA7A54A946F6619D1239AE6A096341CF9A38620295216FAF185554394D80EC04D8B5A7B46EF7FC4618FA645AD46ED4BD7383928B3013693A6F5931E4F5AA7EFCBBA78; pin=%E4%B8%B6%E5%8C%97%E5%9F%8E%E9%B1%BC%E5%AE%89%E7%94%9F; unick=___%E6%A2%A6%E5%AF%90___; ceshi3.com=000; _tp=6KbAcOO79y3rwvtuJvHoHRBSFSbWIqCOxKNAM%2FSmxbwMBsbJ8KsKQWmVHGQC11rU1S2%2FwNGWzgqqKzflVhJMlw%3D%3D; _pst=%E4%B8%B6%E5%8C%97%E5%9F%8E%E9%B1%BC%E5%AE%89%E7%94%9F; shshshsID=8ed6a639183b7cc4d65115ddd4ec2c4b_5_1597199326879; __jdb=122270672.8.1568007721111834248469|61.1597198601; JSESSIONID=C4161CA90DF9781E4CF679DB423DF2D6.s1' } res = requests.get(url=url,params=params,headers=headers) print(res.status_code) # 返回的内容并不是规范的json 格式文件, 利用正则表达式将其转化为规范的 json 文件 html_json = re.search('(? 参考 B站 ：龙王山小青椒 python京东评论抓取-分词-词云展示 Update time： 2020-08-15 "},"案例/爬取网易云音乐.html":{"url":"案例/爬取网易云音乐.html","title":"爬取网易云音乐","keywords":"","body":"爬取网易云音乐 打开 网易云音乐，打开一个歌单： F12 查看网页源代码，我们利用传统的方式查看网页源代码的时候发现没有我们想要获取的元素，这个时候我们需要点击 Network 到里面查询我们需要的数据。在 Doc 下我们可以看到有一个 playlist？id的文件和 song?id= 的文件 , 尝试着请求第一个文件来获取歌曲的id。 获取列表音乐的 id。 import requests from lxml import etree import re url = 'https://music.163.com/playlist?id=888163670' headers = { 'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'referer':'https://music.163.com/' } page = requests.get(url,headers=headers) page.text 获取到页面后，我们可以利用正则表达式或者是 xpath 语法过滤我们需要的内容。 利用正则表达式： pat = re.compile(r'/song\\?id=(\\d+)\">(.*?)') pat.findall(page.text) 结果： [('1468192805', '世界上不存在的歌 (2020重唱版)'), ('1468738841', 'Stay With You (英文版)'), ('1469601898', '粉红色的回忆'), ('1468507490', '沙发里有沙发Radio'), ('1468158074', '梦剧院'), ('1468998724', '清风徐来 (2020重唱版)'), ('1469628663', 'RAPSTAR'), ('1469058069', '闲野人间'), ('1469041281', '很久很久'), ('1463458838', '飒')] 当我们点开列表中的其中一首歌的时候。 同样我们在网页源代码中也可以看到同样的 url (通常利用此处的 Request URL ) 可以看到每首歌都有自己的 id , 我们可以根据上一步获取的 id 来获取每一首歌，但是有了每首歌的 url 后，我们是不能下载的，如何下载呐？ 解决方案：利用外链转换工具，将每首歌的 url 转化为可下载的链接： 外链转换工具：https://link.hhtjim.com/ 我们将上面歌曲的 url 粘贴到里面进行提交，可以看到它给我们生成了一个外链， 将生成的外链复制到浏览器，获取到： 同样分析可以得到：不同歌曲生成的外链只是后面 id 部分不同。 获取音乐的下载链接： wl = 'https://link.hhtjim.com/163/{}.mp3' for id, name in pat.findall(page.text): url = wl.format(id) print(url,name) https://link.hhtjim.com/163/1468192805.mp3 世界上不存在的歌 (2020重唱版) https://link.hhtjim.com/163/1468738841.mp3 Stay With You (英文版) https://link.hhtjim.com/163/1469601898.mp3 粉红色的回忆 https://link.hhtjim.com/163/1468507490.mp3 沙发里有沙发Radio https://link.hhtjim.com/163/1468158074.mp3 梦剧院 https://link.hhtjim.com/163/1468998724.mp3 清风徐来 (2020重唱版) https://link.hhtjim.com/163/1469628663.mp3 RAPSTAR https://link.hhtjim.com/163/1469058069.mp3 闲野人间 https://link.hhtjim.com/163/1469041281.mp3 很久很久 https://link.hhtjim.com/163/1463458838.mp3 飒 有了外链我们就可以进行音乐的下载： wl = 'https://link.hhtjim.com/163/{}.mp3' for id, name in pat.findall(page.text): url = wl.format(id) song = requests.get(url,headers=headers).content # 获取音乐的二进制文件 name = name+\".mp3\" with open(f'./网易云音乐/{name}','wb') as file: file.write(song) print(name,'下载完毕') 世界上不存在的歌 (2020重唱版).mp3 下载完毕 Stay With You (英文版).mp3 下载完毕 粉红色的回忆.mp3 下载完毕 沙发里有沙发Radio.mp3 下载完毕 梦剧院.mp3 下载完毕 清风徐来 (2020重唱版).mp3 下载完毕 RAPSTAR.mp3 下载完毕 闲野人间.mp3 下载完毕 很久很久.mp3 下载完毕 飒.mp3 下载完毕 Update time： 2020-08-11 "},"案例/网易云音乐API集合_测试抓取评论.html":{"url":"案例/网易云音乐API集合_测试抓取评论.html","title":"网易云音乐API集合_测试抓取评论","keywords":"","body":"爬取网易云音乐评论 网站分析 右键检查网页，可以看到 发送的是 post 请求； 所传递的参数是经过加密的。 当进行翻页的时候 下一页的标签的 class 和 id 每次刷新的时候是会发生变化的。模拟浏览器的点击进行翻页爬取不太可取。 解决方案：利用 api 接口： API 集合 简单介绍一下它们： 评论 http://music.163.com/api/v1/resource/comments/R_SO_4_{歌曲ID}?limit=20&offset=0 这应该是最最最常见的了，毕竟80%的网易云音乐的爬虫/数据分析文章都是关于评论数据~ 使用技巧： limit：返回数据条数(每页获取的数量)，默认为20，可以自行更改 offset：偏移量(翻页)，offset需要是limit的倍数 type：搜索的类型 举例，比如limit设置为10，则第一页，第二页分别为： http://music.163.com/api/v1/resource/comments/R_SO_4_483671599?limit=10&offset=0 http://music.163.com/api/v1/resource/comments/R_SO_4_483671599?limit=10&offset=10 PS:返回的数据格式为 json，需要注意的是通过此接口获取的评论数量最多2万条。 歌单 https://music.163.com/api/playlist/detail?id={歌单ID} 网易云音乐每日推荐各种神奇歌单也是它的一大特色，我们可以利用这个api获取歌单里的所有歌曲信息。 https://music.163.com/api/playlist/detail?id=2557908184 同时歌单api还可以应用于各种榜单上，例如： id=19723756，云音乐飙升榜 id=3779629，云音乐新歌榜 id=3778678，云音乐热歌榜 id=2250011882，抖音排行榜 具体id可以按需求自己查找。 歌曲下载 外链： https://link.hhtjim.com/163/{歌曲ID}.mp3 用户信息 https://music.163.com/api/v1/user/detail/{用户ID} 大家在获取到了评论之后，也会同时得到该条评论的用户id。 那么利用他的id和这个用户信息api来获取用户的信息。 汇总之后，我们就可以得到一个歌手在网易云的粉丝用户画像。 歌词 https://music.163.com/api/song/lyric?id={歌曲ID}&lv=1&kv=1&tv=-1 歌词用来做什么呢？ 随便举例几个标题： 《周杰伦14张专辑歌词，里面是19年的岁月》 《华语歌坛30年，大家都在唱些什么？》 搜索结果 http://music.163.com/api/search/get/web?csrf_token=hlpretag=&hlposttag=&s={搜索内容}&type=1&offset=0&total=true&limit=20 使用技巧： limit：返回数据条数（每页获取的数量），默认为20，可以自行更改 offset：偏移量（翻页），offset需要是limit的倍数 type：搜索的类型 type=1 单曲 type=10 专辑 type=100 歌手 type=1000 歌单 type=1002 用户 type=1004 MV type=1006 歌词 type=1009 主播电台 其它 最后推荐一些冷门的接口： 歌手专辑 http://music.163.com/api/artist/albums/{歌手ID}?id={歌手ID}&offset=0&total=true&limit=10 专辑信息 http://music.163.com/api/album/{专辑ID}?ext=true&id={专辑ID}&offset=0&total=true&limit=10 歌曲信息 http://music.163.com/api/song/detail/?id={歌曲ID}&ids=%5B{歌曲ID}%5D MV http://music.163.com/api/mv/detail?id={MV的ID}&type=mp4 测试抓取评论 import pandas as pd import requests import json import time song_id = 1425626819 offset = '' headers = { 'content-type': 'application/x-www-form-urlencoded', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'cookie': '_iuqxldmzr_=32; _ntes_nnid=1b13c21d3407623f34a2b5cfc6f5fcba,1569725214357; _ntes_nuid=1b13c21d3407623f34a2b5cfc6f5fcba; WM_TID=DpS97TOR3gFBRRUBAUJ5pmrJrNwUZA8r; mail_psc_fingerprint=8bb6306983fdc5e6fa2e5c561ac169eb; usertrack=ezq0ZV3IzClwXAVSGNFXAg==; _ga=GA1.2.208351822.1573440559; P_INFO=hugg20171226@163.com|1594624326|0|mail163|00&99|jis&1594622860&mail163#jis&320100#10#0#0|&0|mail163_qrcode&mailmasterpro|hugg20171226@163.com; _antanalysis_s_id=1597020289916; playerid=46996488; JSESSIONID-WYYY=Mc68a9vEj1SCScO8P5YG%2BCiFzkdeE877wZIMk3Fq2FbhFfxaR81TuoeQbUmU%2FHpOQMEoYECEbynlYfPwc1iwFac3uCJFJX5Czwq60TZN3rEVfKYokuvDx1R6UWM7pBckqjqmMHHhX7dxNgORFOziOeTKNMOHjoFsYV7e3%2FEHRyBA7YYX%3A1597234685255; WM_NI=o6ItRe8GXmBTCymtZL1cklzEOOnUDNjObyRlUwHaDrSSTlP9HBf0lCDqrwFA1R2nVCz54LQl4%2FEeWEn1iJQYxxV0ziQuTdVXanG8Qcu5%2BPQB0WM00RcnWMhOAnCIAlKqZEs%3D; WM_NIKE=9ca17ae2e6ffcda170e2e6eeafee74af91aaabae53b0968fb2d15b828a9a84b54db3b8acadb67bb090bedad52af0fea7c3b92aadea8795d16ffc99988ec16081a7a395d948f3bda58df94f8d91a2b6c746aa91ae9ad16897a98882c77a918d9c8dd3338c9eaad1ef4988afbe98c54bf38efe8fd549baebfcadd468f8f18b86d64faeefa7abd453bca89ad1f96393ab9c85d554a3b289d9d469b7ebe5b1d44af48983d1b35aa1b299aeeb4583b28daac46889af96a6f237e2a3; MUSIC_U=dec0269e7e4782a307d0d827a242f6d08afbbb3220811feaddbf82666042164a33a649814e309366; __csrf=04f1a8908510c256be53ed7b7b0d735c; ntes_kaola_ad=1' } for offset in range(200): # 关键 url url = f'http://music.163.com/api/v1/resource/comments/R_SO_4_{song_id}?limit=20&offset={offset * 20}' print(url) html = requests.post(url, headers=headers) # print(html.text) j = json.loads(html.text) # print(j.keys()) for num in range(len(j['comments'])): # num 第几条评论 comment = [(j['comments'][num]['user']['nickname'] , j['comments'][num]['content'] , j['comments'][num]['likedCount'] , j['comments'][num]['time'] )] comment = pd.DataFrame(comment) comment.to_csv('wyyl.csv', header=False, index=False, mode='a+') time.sleep(2) print('page ' + str(1 + offset) + \" has done\") 结果： 参考 收藏这些API，获取网易云音乐数据超轻松 网易云音乐评论爬虫（三）:爬取歌曲的全部评论 https://github.com/zyingzhou/music163-spiders/blob/master/get_comments.py Update time： 2020-08-12 "},"案例/B站弹幕爬取.html":{"url":"案例/B站弹幕爬取.html","title":"B站弹幕爬取","keywords":"","body":"B站弹幕爬取 主要的流程： 根据 BV(或av) 号获取 cid。 根据 cid 请求弹幕。 找到要爬取的视频，检查网页源代码： 找到如图所示的文件：可以获取到 视频的 cid 或者： 我们通过上面文件的 Request URL URL: 来发送请求获取相关的参数： import requests import re import chardet url ='https://api.bilibili.com/x/player/pagelist?bvid=BV1cJ411E7Bd&jsonp=jsonp' page = requests.get(url) page.text 结果： 获取视频的 cid cid = re.findall(r'\"cid\":(\\d+)',page.text)[0] # '143231777' 根据 cid 请求弹幕文件 这部分内容是参考别人的，可能是b站把这个文件给改掉了，找不到这个文件了，但是这个接口还是可以用的 bilibili的弹幕是在xml文件里，每个视频都有其对应的 cid 和 aid，我们取到 cid 中的数字放入http://comment.bilibili.com/+cid+.xml,即可得到该视频对应的cid。 比如：打开这个链接http://comment.bilibili.com/143231777.xml，就可以看到： 之前查找请求弹幕 url 的方式： 首先找到网页请求弹幕的 url 有了接口和 cid 后我们就可以发送请求，获取弹幕的 xml 文件。 dm_base_url = 'http://comment.bilibili.com/{}.xml' dm_url = dm_base_url.format(cid) dm_url # http://comment.bilibili.com/143231777.xml 将上面的 url 复制到浏览器可以看到弹幕文件： 请求弹幕文件： # 请求网页数据 dm_page = requests.get(dm_url) # 将获取的文件进行解码 code = chardet.detect(dm_page.content)['encoding'] # 'utf-8' dm=dm_page.content.decode(code) 提取弹幕 根据弹幕在浏览器中的显示我们可以看到，弹幕内容位于 d 标签内部。利用正则表达式来提取弹幕内容。 dm_list = re.findall(r'(.*?)',dm) dm_list[:10] 这个文件总的弹幕数： len(dm_list) # 3000 这里只是获取了 3000 条弹幕。 如何获取全部（历史）弹幕呢？ 当一个视频都几个部分，如何获取弹幕呢？ 获取历史弹幕需要登录用户账号 可以看到上面的视频包含 4 P，根据上面方式我们查看网页源代码，找相应的文件： 可以看到这四个视频分别对应四个 cid 。 根据不同的 cid 我们就可以获取不同视频对应的弹幕了。 默认爬取的是最近的 3000 条。但是如何获取历史弹幕呢？ 查网页源代码 点击一个左边的一个日期： 可以看到新加载一个 的文件。 点击这文件 ，复制 Request URL 到浏览器：https://api.bilibili.com/x/v2/dm/history?type=1&oid=202042220&date=2020-08-09 可以看到是存储弹幕的 xml 文件。当前文件有 6000 条弹幕。 分析这个 url :https://api.bilibili.com/x/v2/dm/history?type=1&oid=202042220&date=2020-08-09 后面主要有两个参数：cid(oid) 和 date ; 通过调整后面的日期我们就可以获取当前视频不同时间的弹幕。 可以看到不同时间的弹幕是有重复的 对于重的 可以通过 p 标签的内容进行过滤。 也可以根据请求到的数据获取含有弹幕的日期： 这个文件会返回一个日期列表：当前月份几号是有弹幕的。例如：https://api.bilibili.com/x/v2/dm/history/index?type=1&oid=202042220&month=2020-08 表示今年 8 月份。将上面的 url 复制到浏览器： 可以看到这些天数是有弹幕的，改变月份： 但是利用获取到的日期去查询弹幕，好像也不能解决后面时间段，不同文件含有弹幕重复的问题。 一种解决方法就是根据弹幕文件中的 p 标签的内容进行过滤： 可以百度 B 站 弹幕文件参数 Update time： 2020-08-11 "},"案例/Pandas爬取表格数据.html":{"url":"案例/Pandas爬取表格数据.html","title":"Pandas爬取表格数据","keywords":"","body":"Pandas爬取表格数据 table型表格 我们在网页上会经常看到这样一些表格，比如： 从中可以看到table类型的表格网页结构大致如下： ... ... ... ... ... ... ... ... ... ... ... 先来简单解释一下上文出现的几种标签含义： : 定义表格 : 定义表格的页眉 : 定义表格的主体 : 定义表格的行 : 定义表格的表头 : 定义表格单元 这样的表格数据，就可以利用pandas模块里的read_html函数方便快捷地抓取下来。下面我们就来操作一下。 快速抓取 下面以中国上市公司信息这个网页中的表格为例，感受一下read_html函数的强大之处。 import pandas as pd writer = pd.ExcelWriter(\"股票.xlsx\", engine='openpyxl', mode='a') for i in range(1,178): # 爬取全部177页数据 url = 'http://s.askci.com/stock/a/?reportTime=2018-06-30&pageNum=%s' % (str(i)) tb = pd.read_html(url)[3] #经观察发现所需表格是网页中第4个表格，故为[3] tb.to_excel(writer, sheet_name=str(i), index=False) print('第'+str(i)+'页抓取完成') writer.save() writer.close() 只需不到十行代码，1分钟左右就可以将全部178页共3535家A股上市公司的信息干净整齐地抓取下来。比采用正则表达式、xpath这类常规方法要省心省力地多。如果采取人工一页页地复制粘贴到excel中，就得操作到猴年马月去了。 详细代码实现 read_html函数 先来了解一下read_html函数的api: def read_html( io, match=\".+\", flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False, thousands=\",\", encoding=None, decimal=\".\", converters=None, na_values=None, keep_default_na=True, displayed_only=True, ): r\"\"\" Read HTML tables into a ``list`` of ``DataFrame`` objects. Parameters ---------- io : str, path object or file-like object A URL, a file-like object, or a raw string containing HTML. Note that lxml only accepts the http, ftp and file url protocols. If you have a URL that starts with ``'https'`` you might try removing the ``'s'``. match : str or compiled regular expression, optional The set of tables containing text matching this regex or string will be returned. Unless the HTML is extremely simple you will probably need to pass a non-empty string here. Defaults to '.+' (match any non-empty string). The default value will return all tables contained on a page. This value is converted to a regular expression so that there is consistent behavior between Beautiful Soup and lxml. flavor : str or None The parsing engine to use. 'bs4' and 'html5lib' are synonymous with each other, they are both there for backwards compatibility. The default of ``None`` tries to use ``lxml`` to parse and if that fails it falls back on ``bs4`` + ``html5lib``. header : int or list-like or None, optional The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to make the columns headers. index_col : int or list-like or None, optional The column (or list of columns) to use to create the index. skiprows : int or list-like or slice or None, optional Number of rows to skip after parsing the column integer. 0-based. If a sequence of integers or a slice is given, will skip the rows indexed by that sequence. Note that a single element sequence means 'skip the nth row' whereas an integer means 'skip n rows'. attrs : dict or None, optional This is a dictionary of attributes that you can pass to use to identify the table in the HTML. These are not checked for validity before being passed to lxml or Beautiful Soup. However, these attributes must be valid HTML table attributes to work correctly. For example, :: attrs = {'id': 'table'} is a valid attribute dictionary because the 'id' HTML tag attribute is a valid HTML attribute for *any* HTML tag as per `this document `__. :: attrs = {'asdf': 'table'} is *not* a valid attribute dictionary because 'asdf' is not a valid HTML attribute even if it is a valid XML attribute. Valid HTML 4.01 table attributes can be found `here `__. A working draft of the HTML 5 spec can be found `here `__. It contains the latest information on table attributes for the modern web. parse_dates : bool, optional See :func:`~read_csv` for more details. thousands : str, optional Separator to use to parse thousands. Defaults to ``','``. encoding : str or None, optional The encoding used to decode the web page. Defaults to ``None``.``None`` preserves the previous encoding behavior, which depends on the underlying parser library (e.g., the parser library will try to use the encoding provided by the document). decimal : str, default '.' Character to recognize as decimal point (e.g. use ',' for European data). converters : dict, default None Dict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the cell (not column) content, and return the transformed content. na_values : iterable, default None Custom NA values. keep_default_na : bool, default True If na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they're appended to. displayed_only : bool, default True Whether elements with \"display: none\" should be parsed. io：接收网址，文件，字符串。网址不接受https，尝试去掉s后爬取 match：正则表达式，返回与正则表达式匹配的表格。默认\".+\" flavor：解析器。默认为(“lxml”,“bs4”) header：指定列标题所在的行 index_col：指定行标题对应的列 skiprows：跳过第n行 attrs：传递一个字典，标示表格的属性值 parse_dates：解析日期 thousands：千位分隔符 encoding：编码方式 decimal：小数点指示，默认使用\".\" converters：转换某些列的函数的字典 na_values：标示那些为NA的值 keep_default_na：保持默认的NA值，与na_values一起使用 displayed_only：是否应解析具有\"display:none\"的元素。默认为True 缺点：如果网站需要使用用户代理(User_Agent)，那么read_html就无法爬取成功 参考 python可视化1--DataFrame.read_html函数使用 pandas之read_html爬虫 Update time： 2020-08-15 "},"案例/有道翻译案例.html":{"url":"案例/有道翻译案例.html","title":"有道翻译案例","keywords":"","body":"有道翻译案例 有道翻译界面： 当我们进行语言翻译的时候，可以注意到浏览器的 url 并不会发生改变，这种是前端的 Ajax 技术。那么如何利用 python 爬虫解决这样的问题呢？ 右键检查网页，Network 下面的 XHR (Xml Http Request) 首先清空，在左侧的翻译处输入文字，观察右侧文件的变化； 输入完文字回车后，可以看到右侧记载了一个新文件，点击查看文件 返回的内容 当我们输入完文字后，发送一个 POST 请求，返回的文件内容如上图所示，我们可以根据上面的 Request URL 和Form data 参数发送请求，接收并过滤返回的内容。 test : 可以看到返回的状态是对的。但是文本内容是错的，正确返回应该是{\"errorCode\":50} , 是上面原因呢 ？ 在网上查询之后 重新测试： 完全可以 ！ 完整代码： Update time： 2020-08-11 "},"案例/移动端微博评论抓取.html":{"url":"案例/移动端微博评论抓取.html","title":"移动端微博评论抓取","keywords":"","body":"移动端微博评论抓取 Update time： 2020-08-12 "},"案例/豆果网数据爬取.html":{"url":"案例/豆果网数据爬取.html","title":"豆果网数据爬取","keywords":"","body":"豆果网数据爬取 常规网页的静态爬虫。 利用 csv 文件的字典格式 csv.DictWriter 存储数据 # -*- coding = uft-8 -*- import csv import time import random import requests import traceback from lxml import etree # 获取首页源码 def get_page(url): n = 3 while True: try: # sleep(random.uniform(1, 2)) # 随机出现1-2之间的数，包含小数 headers = { 'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" } response = requests.get(url, headers=headers, timeout=10) # print(response.text) return response.text except (TimeoutError, Exception): n -= 1 if n == 0: print('请求3次均失败，放弃此url请求,检查请求条件') return else: print('请求失败，重新请求') continue # 爬取一页数据 def parse_page(html, caipu): try: parse = etree.HTML(html) # 解析网页 items = parse.xpath('//li[@class=\"clearfix\"]') for item in items: title = ''.join(item.xpath('./a/@title')).strip() href = 'https://www.douguo.com' + ''.join(item.xpath('./div/a/@href')).strip() peiliao = ''.join(item.xpath('./div/p/text()')).strip() rate = ''.join(item.xpath('./div/div[1]/span[2]/text()')).strip() id = ''.join(item.xpath('./div/div[2]/a[1]/text()')).strip() img = ''.join(item.xpath('./a/img/@src')).strip() item = { 'title': title, 'href': href, 'peiliao': peiliao, 'rate': rate, 'id': id, 'img': img, 'caipu': caipu } # print(item) try: with open('./caipu.csv', 'a', encoding='utf_8_sig', newline='') as fp: # 'a'为追加模式（添加） # utf_8_sig格式导出csv不乱码 fieldnames = ['title', 'href', 'peiliao', 'rate', 'id', 'img', 'caipu'] writer = csv.DictWriter(fp, fieldnames) writer.writerow(item) except Exception: print(traceback.print_exc()) # 代替print e 来输出详细的异常信息 except Exception: print(traceback.print_exc()) # 主函数 def main(x): url = 'https://www.douguo.com/caipu/{}/0/{}'.format(caipu, x * 20) print(url) html = get_page(url) parse_page(html, caipu) if __name__ == '__main__': caipu_list = ['川菜', '湘菜', '粤菜', '东北菜', '鲁菜', '浙菜', '湖北菜', '清真菜'] # 中国菜系 start = time.time() # 计时 for caipu in caipu_list: for i in range(22): # 爬取多页 main(x=i) time.sleep(random.uniform(1, 2)) print(caipu, \"第\" + str(i + 1) + \"页提取完成\") end = time.time() print('共用时', round((end - start) / 60, 2), '分钟') Update time： 2020-08-12 "}}